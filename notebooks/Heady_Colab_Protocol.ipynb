{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Heady Systems - Project Quickstart\n",
        "\n",
        "```\n",
        "\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
        "\u2551  \u2588\u2588\u2557  \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557   \u2588\u2588\u2557  \u2551\n",
        "\u2551  \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u255a\u2588\u2588\u2557 \u2588\u2588\u2554\u255d  \u2551\n",
        "\u2551  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2554\u255d   \u2551\n",
        "\u2551  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d  \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551  \u2588\u2588\u2551  \u255a\u2588\u2588\u2554\u255d    \u2551\n",
        "\u2551  \u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d   \u2588\u2588\u2551    \u2551\n",
        "\u2551  \u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u255d    \u255a\u2550\u255d    \u2551\n",
        "\u2551                                               \u2551\n",
        "\u2551  \u221e SACRED GEOMETRY \u221e                            \u2551\n",
        "\u2551  Organic Systems \u00b7 Breathing Interfaces         \u2551\n",
        "\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
        "```\n",
        "\n",
        "**Version:** 2.0.0 | **Runtime:** GPU (T4 recommended) | **Repo:** [HeadySystems/Heady](https://github.com/HeadySystems/Heady)\n",
        "\n",
        "---\n",
        "\n",
        "## What This Notebook Does\n",
        "\n",
        "| Step | Description |\n",
        "|------|-------------|\n",
        "| **1. Environment** | Verify GPU, install system deps |\n",
        "| **2. Clone & Install** | Pull repo, install Node.js + Python packages |\n",
        "| **3. Configuration** | Set API keys and environment variables |\n",
        "| **4. Boot Manager** | Start `heady-manager.js` on port 3300 |\n",
        "| **5. API Explorer** | Hit health, registry, conductor, and pipeline endpoints |\n",
        "| **6. HuggingFace Inference** | Run PYTHIA node with GPU-accelerated models |\n",
        "| **7. Tunnel Access** | Expose the Sacred Geometry UI via ngrok |\n",
        "| **8. Quickstart Quiz** | Flashcard review of core architecture |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Environment Verification\n",
        "Confirm GPU availability and print system info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import subprocess, sys, os\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"HEADY SYSTEMS - Environment Check\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# GPU check\n",
        "gpu_available = torch.cuda.is_available()\n",
        "gpu_name = torch.cuda.get_device_name(0) if gpu_available else \"None\"\n",
        "gpu_mem = f\"{torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\" if gpu_available else \"N/A\"\n",
        "\n",
        "print(f\"GPU Available : {gpu_available}\")\n",
        "print(f\"GPU Device    : {gpu_name}\")\n",
        "print(f\"GPU Memory    : {gpu_mem}\")\n",
        "print(f\"PyTorch       : {torch.__version__}\")\n",
        "print(f\"CUDA Version  : {torch.version.cuda}\")\n",
        "print(f\"Python        : {sys.version.split()[0]}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if not gpu_available:\n",
        "    print(\"\\n[WARNING] No GPU detected.\")\n",
        "    print(\"Go to Runtime > Change runtime type > GPU\")\n",
        "else:\n",
        "    print(f\"\\n[OK] GPU ready: {gpu_name} ({gpu_mem})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Clone Repository & Install Dependencies\n",
        "Pull the Heady repo and install both Node.js and Python packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "echo \"[1/4] Installing Node.js 20.x...\"\n",
        "curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash - > /dev/null 2>&1\n",
        "sudo apt-get install -y nodejs > /dev/null 2>&1\n",
        "echo \"  Node: $(node --version)  npm: $(npm --version)\"\n",
        "\n",
        "echo \"[2/4] Cloning Heady repository...\"\n",
        "if [ -d \"/content/Heady\" ]; then\n",
        "  echo \"  Repo already exists, pulling latest...\"\n",
        "  cd /content/Heady && git pull --ff-only\n",
        "else\n",
        "  git clone https://github.com/HeadySystems/Heady.git /content/Heady\n",
        "fi\n",
        "\n",
        "echo \"[3/4] Installing Node.js dependencies...\"\n",
        "cd /content/Heady && npm install --production 2>&1 | tail -1\n",
        "\n",
        "echo \"[4/4] Installing Python dependencies...\"\n",
        "pip install -q -r /content/Heady/backend/python_worker/requirements.txt 2>&1 | tail -3\n",
        "\n",
        "echo \"\"\n",
        "echo \"[OK] All dependencies installed.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Configuration\n",
        "Set environment variables. Replace placeholder values with your actual keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# --- Required ---\n",
        "# You can paste keys directly or use getpass for security\n",
        "os.environ[\"HEADY_API_KEY\"] = getpass(\"Enter HEADY_API_KEY (or press Enter for dev default): \") or \"dev_colab_key\"\n",
        "os.environ[\"ADMIN_TOKEN\"] = getpass(\"Enter ADMIN_TOKEN (or press Enter for dev default): \") or \"colab_secure_token\"\n",
        "\n",
        "# --- Optional: Hugging Face ---\n",
        "hf_token = getpass(\"Enter HF_TOKEN (or press Enter to skip): \")\n",
        "if hf_token:\n",
        "    os.environ[\"HF_TOKEN\"] = hf_token\n",
        "\n",
        "# --- System config ---\n",
        "os.environ[\"PORT\"] = \"3300\"\n",
        "os.environ[\"NODE_ENV\"] = \"development\"\n",
        "os.environ[\"HEADY_TARGET\"] = \"colab\"\n",
        "os.environ[\"HEADY_VERSION\"] = \"2.0.0-colab\"\n",
        "os.environ[\"ENABLE_CODEMAP\"] = \"true\"\n",
        "os.environ[\"JULES_ENABLED\"] = \"true\"\n",
        "os.environ[\"OBSERVER_ENABLED\"] = \"true\"\n",
        "os.environ[\"BUILDER_ENABLED\"] = \"true\"\n",
        "os.environ[\"ATLAS_ENABLED\"] = \"true\"\n",
        "\n",
        "# GPU config (auto-detect)\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    os.environ[\"HEADY_ADMIN_ENABLE_GPU\"] = \"true\"\n",
        "    gpu_mem_mb = int(torch.cuda.get_device_properties(0).total_mem / 1e6)\n",
        "    os.environ[\"GPU_MEMORY_LIMIT\"] = str(gpu_mem_mb)\n",
        "    print(f\"[OK] GPU config set: {gpu_mem_mb} MB\")\n",
        "else:\n",
        "    os.environ[\"HEADY_ADMIN_ENABLE_GPU\"] = \"false\"\n",
        "    print(\"[INFO] Running in CPU-only mode\")\n",
        "\n",
        "print(f\"[OK] Environment configured for Heady {os.environ['HEADY_VERSION']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Boot Heady Manager\n",
        "Start the Node.js MCP server in the background on port 3300."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess, time, os, json\n",
        "from urllib.request import urlopen, Request\n",
        "from urllib.error import URLError\n",
        "\n",
        "HEADY_DIR = \"/content/Heady\"\n",
        "PORT = int(os.environ.get(\"PORT\", 3300))\n",
        "\n",
        "# Build env dict for the subprocess\n",
        "env = {**os.environ, \"PORT\": str(PORT)}\n",
        "\n",
        "# Start heady-manager.js in background\n",
        "print(f\"[BOOT] Starting heady-manager.js on port {PORT}...\")\n",
        "proc = subprocess.Popen(\n",
        "    [\"node\", \"heady-manager.js\"],\n",
        "    cwd=HEADY_DIR,\n",
        "    env=env,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT\n",
        ")\n",
        "\n",
        "# Wait for server to be ready\n",
        "health_url = f\"http://localhost:{PORT}/api/health\"\n",
        "for attempt in range(15):\n",
        "    time.sleep(2)\n",
        "    try:\n",
        "        resp = urlopen(health_url, timeout=3)\n",
        "        data = json.loads(resp.read())\n",
        "        if data.get(\"ok\"):\n",
        "            print(f\"[OK] Heady Manager is LIVE (PID: {proc.pid})\")\n",
        "            print(f\"     Service : {data.get('service')}\")\n",
        "            print(f\"     Version : {data.get('version')}\")\n",
        "            print(f\"     Health  : http://localhost:{PORT}/api/health\")\n",
        "            break\n",
        "    except (URLError, Exception):\n",
        "        if attempt < 14:\n",
        "            print(f\"     Waiting... ({attempt+1}/15)\")\n",
        "        else:\n",
        "            print(\"[ERROR] Server failed to start. Check logs:\")\n",
        "            print(proc.stdout.read(4096).decode())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. API Explorer\n",
        "Interact with the Heady Manager REST API. Each cell hits a different endpoint group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from urllib.request import urlopen, Request\n",
        "\n",
        "BASE = f\"http://localhost:{os.environ.get('PORT', 3300)}\"\n",
        "\n",
        "def heady_get(path):\n",
        "    \"\"\"GET a Heady API endpoint and pretty-print the JSON response.\"\"\"\n",
        "    url = f\"{BASE}{path}\"\n",
        "    try:\n",
        "        resp = urlopen(url, timeout=10)\n",
        "        data = json.loads(resp.read())\n",
        "        print(f\"GET {path}\")\n",
        "        print(json.dumps(data, indent=2))\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def heady_post(path, body=None):\n",
        "    \"\"\"POST to a Heady API endpoint and pretty-print the JSON response.\"\"\"\n",
        "    url = f\"{BASE}{path}\"\n",
        "    try:\n",
        "        payload = json.dumps(body or {}).encode()\n",
        "        req = Request(url, data=payload, headers={\"Content-Type\": \"application/json\"})\n",
        "        resp = urlopen(req, timeout=30)\n",
        "        data = json.loads(resp.read())\n",
        "        print(f\"POST {path}\")\n",
        "        print(json.dumps(data, indent=2))\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"Helper functions loaded: heady_get(path), heady_post(path, body)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5a. Health & Pulse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Health check\n",
        "heady_get(\"/api/health\")\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "# System pulse\n",
        "heady_get(\"/api/pulse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5b. Registry & Nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Component registry\n",
        "heady_get(\"/api/registry\")\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "# Node status\n",
        "heady_get(\"/api/nodes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5c. System Status & Production Activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full system status\n",
        "status = heady_get(\"/api/system/status\")\n",
        "\n",
        "if status:\n",
        "    caps = status.get(\"capabilities\", {})\n",
        "    print(\"\\n--- Capability Summary ---\")\n",
        "    for key, val in caps.items():\n",
        "        total = val.get(\"total\", 0)\n",
        "        active = val.get(\"active\", val.get(\"healthy\", 0))\n",
        "        print(f\"  {key:12s}: {active}/{total} active\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Activate production mode (enables all nodes, tools, workflows)\n",
        "# Uncomment the next line to activate:\n",
        "# heady_post(\"/api/system/production\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5d. Pipeline & Subsystems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline config\n",
        "heady_get(\"/api/pipeline/config\")\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "# Pipeline state\n",
        "heady_get(\"/api/pipeline/state\")\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "# Brain status\n",
        "heady_get(\"/api/brain/status\")\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "# Readiness evaluation\n",
        "heady_get(\"/api/readiness/evaluate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5e. Layer Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Current layer\n",
        "heady_get(\"/api/layer\")\n",
        "\n",
        "# Available layers:\n",
        "#   local      - Local Dev (localhost:3300)\n",
        "#   cloud-me   - Cloud HeadyMe\n",
        "#   cloud-sys  - Cloud HeadySystems\n",
        "#   cloud-conn - Cloud HeadyConnection\n",
        "#   hybrid     - Hybrid Local+Cloud\n",
        "#\n",
        "# Switch layer (uncomment to use):\n",
        "# heady_post(\"/api/layer/switch\", {\"layer\": \"cloud-sys\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. HeadyConductor - Python Orchestration\n",
        "Run the HeadyConductor directly to orchestrate AI nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess, json\n",
        "\n",
        "HEADY_DIR = \"/content/Heady\"\n",
        "CONDUCTOR = f\"{HEADY_DIR}/HeadyAcademy/HeadyConductor.py\"\n",
        "\n",
        "def run_conductor(args: list):\n",
        "    \"\"\"Run HeadyConductor with given arguments and return parsed output.\"\"\"\n",
        "    result = subprocess.run(\n",
        "        [\"python\", CONDUCTOR] + args,\n",
        "        capture_output=True, text=True, cwd=HEADY_DIR,\n",
        "        env={**os.environ, \"PYTHONUNBUFFERED\": \"1\"}\n",
        "    )\n",
        "    print(f\"$ python HeadyConductor.py {' '.join(args)}\")\n",
        "    if result.returncode != 0:\n",
        "        print(f\"[ERROR] Exit code {result.returncode}\")\n",
        "        print(result.stderr[-500:] if result.stderr else \"No stderr\")\n",
        "        return None\n",
        "    # Try to extract JSON\n",
        "    import re\n",
        "    match = re.search(r'\\{[\\s\\S]*\\}', result.stdout)\n",
        "    if match:\n",
        "        data = json.loads(match.group(0))\n",
        "        print(json.dumps(data, indent=2))\n",
        "        return data\n",
        "    else:\n",
        "        print(result.stdout[-1000:])\n",
        "        return result.stdout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System summary\n",
        "run_conductor([\"--summary\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Health check via Conductor\n",
        "run_conductor([\"--health\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Orchestrate a request (routes to the appropriate AI node)\n",
        "# Example: Ask PYTHIA for inference\n",
        "run_conductor([\"--request\", \"analyze the system architecture\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. HuggingFace Inference (GPU-Accelerated)\n",
        "Run the PYTHIA node's HuggingFace tool directly with GPU acceleration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Using device: {'GPU (cuda:0)' if device == 0 else 'CPU'}\")\n",
        "\n",
        "# Text generation with a small model\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\", device=device)\n",
        "\n",
        "prompt = \"Heady Systems uses Sacred Geometry architecture to\"\n",
        "result = generator(prompt, max_length=80, num_return_sequences=1, do_sample=True, temperature=0.7)\n",
        "\n",
        "print(\"\\n--- Generated Text ---\")\n",
        "print(result[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentiment analysis\n",
        "classifier = pipeline(\"sentiment-analysis\", device=device)\n",
        "\n",
        "texts = [\n",
        "    \"The Sacred Geometry architecture makes everything interconnected and beautiful.\",\n",
        "    \"The system health check failed with critical errors.\",\n",
        "    \"All nodes are active and the pipeline completed successfully.\"\n",
        "]\n",
        "\n",
        "print(\"--- Sentiment Analysis ---\")\n",
        "for text, result in zip(texts, classifier(texts)):\n",
        "    print(f\"  [{result['label']:8s} {result['score']:.3f}] {text[:60]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summarization (uses T5-small, good for Colab GPU)\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-small\", device=device)\n",
        "\n",
        "architecture_text = \"\"\"\n",
        "Heady Systems is a hybrid Node.js and Python system for the HeadyConnection\n",
        "ecosystem with a patented Sacred Geometry architecture. It features a web-based\n",
        "Admin IDE with AI assistance, real-time build and audit monitoring, and optional\n",
        "remote GPU support. The system is orchestrated by heady-manager.js, a Node.js\n",
        "Express MCP server running on port 3300. It manages AI nodes including JULES for\n",
        "code optimization, OBSERVER for monitoring, BUILDER for project construction,\n",
        "ATLAS for documentation, and PYTHIA for predictive inference via HuggingFace.\n",
        "The HeadyConductor routes requests to appropriate nodes based on trigger keywords.\n",
        "The system deploys to Render.com using infrastructure-as-code via render.yaml\n",
        "and supports multiple cloud layers including HeadyMe, HeadySystems, and\n",
        "HeadyConnection endpoints.\n",
        "\"\"\"\n",
        "\n",
        "summary = summarizer(architecture_text, max_length=60, min_length=20)\n",
        "print(\"--- Architecture Summary ---\")\n",
        "print(summary[0][\"summary_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Tunnel Access (ngrok)\n",
        "Expose the Heady UI publicly so you can access the Sacred Geometry dashboard from your browser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install pyngrok\n",
        "!pip install -q pyngrok\n",
        "\n",
        "from getpass import getpass\n",
        "ngrok_token = getpass(\"Enter your ngrok authtoken (from https://dashboard.ngrok.com): \")\n",
        "\n",
        "if ngrok_token:\n",
        "    from pyngrok import ngrok, conf\n",
        "    conf.get_default().auth_token = ngrok_token\n",
        "\n",
        "    PORT = int(os.environ.get(\"PORT\", 3300))\n",
        "    tunnel = ngrok.connect(PORT)\n",
        "    public_url = tunnel.public_url\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"HEADY SYSTEMS - Public Access\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"  Main UI  : {public_url}\")\n",
        "    print(f\"  Admin IDE: {public_url}/admin\")\n",
        "    print(f\"  Health   : {public_url}/api/health\")\n",
        "    print(f\"  Pulse    : {public_url}/api/pulse\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nKeep this notebook running to maintain the tunnel.\")\n",
        "else:\n",
        "    print(\"[SKIP] No ngrok token provided. Access via Colab proxy only.\")\n",
        "    print(f\"  Local URL: http://localhost:{os.environ.get('PORT', 3300)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Run a Pipeline\n",
        "Trigger the HCFullPipeline which runs all stages: Supervisor routing, Brain auto-tuning, Health checks, Readiness evaluation, and Checkpoint analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Trigger pipeline run\n",
        "run_result = heady_post(\"/api/pipeline/run\")\n",
        "\n",
        "if run_result and run_result.get(\"accepted\"):\n",
        "    run_id = run_result[\"runId\"]\n",
        "    print(f\"\\nPipeline started: {run_id}\")\n",
        "    print(\"Polling for completion...\\n\")\n",
        "\n",
        "    for i in range(30):\n",
        "        time.sleep(2)\n",
        "        state = heady_get(\"/api/pipeline/state\") if i % 5 == 0 else None\n",
        "        if state is None:\n",
        "            # Silent poll\n",
        "            import json\n",
        "            from urllib.request import urlopen\n",
        "            try:\n",
        "                resp = urlopen(f\"{BASE}/api/pipeline/state\", timeout=5)\n",
        "                state = json.loads(resp.read())\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        status = state.get(\"status\", \"unknown\") if state else \"unknown\"\n",
        "        if status in [\"completed\", \"failed\", \"idle\"]:\n",
        "            print(f\"\\n[DONE] Pipeline status: {status}\")\n",
        "            if state and state.get(\"metrics\"):\n",
        "                print(f\"  Metrics: {json.dumps(state['metrics'], indent=2)}\")\n",
        "            break\n",
        "    else:\n",
        "        print(\"[TIMEOUT] Pipeline still running after 60s. Check /api/pipeline/state\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 10. Quickstart Quiz - Flashcard Review\n",
        "\n",
        "Per the **Heady Documentation Protocol** (Quiz & Flashcard Methodology), here are the core architecture flashcards.\n",
        "\n",
        "---\n",
        "\n",
        "### Architecture\n",
        "\n",
        "| # | Question | Answer |\n",
        "|---|----------|--------|\n",
        "| 1 | What is the primary orchestrator of the Heady system? | `heady-manager.js` - a Node.js Express MCP server on port 3300 |\n",
        "| 2 | What design philosophy does the Heady UI follow? | **Sacred Geometry** - rounded, organic, breathing interfaces |\n",
        "| 3 | What two runtimes does Heady use? | **Node.js** (manager/API) and **Python** (worker/inference) |\n",
        "| 4 | Where is the infrastructure-as-code deployment defined? | `render.yaml` (Render.com Blueprint) |\n",
        "| 5 | What is the health check endpoint? | `GET /api/health` returns `{\"ok\": true, \"service\": \"heady-manager\"}` |\n",
        "\n",
        "### AI Nodes\n",
        "\n",
        "| # | Question | Answer |\n",
        "|---|----------|--------|\n",
        "| 6 | Name the 5 AI nodes in the system. | **JULES** (Surgeon), **OBSERVER** (Monitor), **BUILDER** (Constructor), **ATLAS** (Archivist), **PYTHIA** (Oracle) |\n",
        "| 7 | Which node handles HuggingFace inference? | **PYTHIA** - The Oracle, triggered by `huggingface`, `predict`, `ask_oracle` |\n",
        "| 8 | What does JULES do? | Code optimization: unused import detection, quality analysis, performance suggestions, security scanning |\n",
        "| 9 | How does HeadyConductor route requests? | Matches trigger keywords in the request to the appropriate AI node's trigger list |\n",
        "| 10 | What tool does ATLAS use? | `auto_doc` - generates API docs, performs code analysis, creates knowledge base entries |\n",
        "\n",
        "### Pipeline & Subsystems\n",
        "\n",
        "| # | Question | Answer |\n",
        "|---|----------|--------|\n",
        "| 11 | What is HCFullPipeline? | The main execution engine with stage DAG, circuit breakers, checkpoints, and event-driven subsystem feedback |\n",
        "| 12 | Name the 5 pipeline subsystems. | **Supervisor**, **Brain**, **CheckpointAnalyzer**, **ReadinessEvaluator**, **HealthRunner** |\n",
        "| 13 | What does the Brain subsystem do? | Auto-tunes system parameters based on error rate, latency, and queue utilization |\n",
        "| 14 | How do you trigger a pipeline run via API? | `POST /api/pipeline/run` - returns immediately with `runId`, runs asynchronously |\n",
        "| 15 | What is a checkpoint in the pipeline? | An analysis point where CheckpointAnalyzer evaluates run state and health, feeding results into Brain |\n",
        "\n",
        "### Cloud Layers\n",
        "\n",
        "| # | Question | Answer |\n",
        "|---|----------|--------|\n",
        "| 16 | What are the 5 cloud layers? | `local`, `cloud-me`, `cloud-sys`, `cloud-conn`, `hybrid` |\n",
        "| 17 | How do you switch layers via API? | `POST /api/layer/switch` with body `{\"layer\": \"cloud-sys\"}` |\n",
        "| 18 | Where is the active layer state persisted? | `scripts/.heady-active-layer` file |\n",
        "| 19 | What does production activation do? | Activates ALL nodes, tools, workflows, services and sets environment to production |\n",
        "| 20 | What endpoint returns the full system status? | `GET /api/system/status` - includes capabilities, layer info, Sacred Geometry state |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive quiz mode\n",
        "import random\n",
        "\n",
        "flashcards = [\n",
        "    (\"What is the primary orchestrator of the Heady system?\",\n",
        "     \"heady-manager.js - a Node.js Express MCP server on port 3300\"),\n",
        "    (\"What design philosophy does the Heady UI follow?\",\n",
        "     \"Sacred Geometry - rounded, organic, breathing interfaces\"),\n",
        "    (\"What two runtimes does Heady use?\",\n",
        "     \"Node.js (manager/API) and Python (worker/inference)\"),\n",
        "    (\"Name the 5 AI nodes.\",\n",
        "     \"JULES (Surgeon), OBSERVER (Monitor), BUILDER (Constructor), ATLAS (Archivist), PYTHIA (Oracle)\"),\n",
        "    (\"Which node handles HuggingFace inference?\",\n",
        "     \"PYTHIA - The Oracle, triggered by: huggingface, predict, ask_oracle\"),\n",
        "    (\"What is HCFullPipeline?\",\n",
        "     \"Main execution engine with stage DAG, circuit breakers, checkpoints, and event-driven feedback\"),\n",
        "    (\"Name the 5 pipeline subsystems.\",\n",
        "     \"Supervisor, Brain, CheckpointAnalyzer, ReadinessEvaluator, HealthRunner\"),\n",
        "    (\"What are the 5 cloud layers?\",\n",
        "     \"local, cloud-me, cloud-sys, cloud-conn, hybrid\"),\n",
        "    (\"How do you activate production mode via API?\",\n",
        "     \"POST /api/system/production - activates all nodes, tools, workflows, services\"),\n",
        "    (\"Where is the infrastructure-as-code deployment defined?\",\n",
        "     \"render.yaml (Render.com Blueprint)\"),\n",
        "]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"HEADY SYSTEMS - Interactive Quiz (10 Questions)\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Press Enter after each question to reveal the answer.\\n\")\n",
        "\n",
        "random.shuffle(flashcards)\n",
        "score = 0\n",
        "\n",
        "for i, (q, a) in enumerate(flashcards, 1):\n",
        "    print(f\"Q{i}: {q}\")\n",
        "    input(\"  [Press Enter to reveal answer]\")\n",
        "    print(f\"  A: {a}\")\n",
        "    got_it = input(\"  Did you know it? (y/n): \").strip().lower()\n",
        "    if got_it == \"y\":\n",
        "        score += 1\n",
        "    print()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "pct = (score / len(flashcards)) * 100\n",
        "print(f\"Score: {score}/{len(flashcards)} ({pct:.0f}%)\")\n",
        "if pct >= 80:\n",
        "    print(\"Sacred Geometry Mastery Achieved!\")\n",
        "elif pct >= 50:\n",
        "    print(\"Good progress. Review the Architecture section above.\")\n",
        "else:\n",
        "    print(\"Re-run the notebook and study the flashcard table in Section 10.\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Cleanup\n",
        "Stop the server and tunnel when done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop ngrok tunnel\n",
        "try:\n",
        "    from pyngrok import ngrok\n",
        "    ngrok.kill()\n",
        "    print(\"[OK] ngrok tunnel closed.\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Stop heady-manager\n",
        "try:\n",
        "    proc.terminate()\n",
        "    proc.wait(timeout=5)\n",
        "    print(f\"[OK] heady-manager stopped (PID: {proc.pid}).\")\n",
        "except:\n",
        "    print(\"[INFO] Server may already be stopped.\")\n",
        "\n",
        "print(\"\\nSession complete. Sacred Geometry resting.\")"
      ]
    }
  ]
}
