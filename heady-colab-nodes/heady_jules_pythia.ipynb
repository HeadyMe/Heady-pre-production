{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JULES + PYTHIA GPU Node — Code Analysis & Reasoning Engine\n",
    "\n",
    "```\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║  ∞ SACRED GEOMETRY ∞  Organic Systems · Breathing Interfaces    ║\n",
    "║  NODE: JULES + PYTHIA Combined (Colab Pro+ GPU)                 ║\n",
    "║  PURPOSE: Code optimization + text generation + reasoning       ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "```\n",
    "\n",
    "**Combined capabilities (2-in-1 GPU node):**\n",
    "- JULES: Code quality analysis & scoring\n",
    "- JULES: Security vulnerability scanning\n",
    "- JULES: Refactoring suggestions via LLM\n",
    "- JULES: Duplicate/similar code detection (CodeBERT)\n",
    "- PYTHIA: Text generation & completion\n",
    "- PYTHIA: Multi-step reasoning chains\n",
    "- PYTHIA: Summarization, sentiment, classification\n",
    "\n",
    "**Branded domains only:** headysystems.com | headycloud.com | headyconnection.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies + verify GPU\n",
    "!pip install -q transformers accelerate sentence-transformers fastapi uvicorn pyngrok httpx nest_asyncio scikit-learn\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('WARNING: No GPU — will run on CPU (slower)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "import os\n",
    "\n",
    "HEADY_CONFIG = {\n",
    "    'node_id': 'jules-pythia-gpu',\n",
    "    'node_role': 'code-optimizer-reasoning-engine',\n",
    "    'port': 5001,\n",
    "    'cloud_layers': {\n",
    "        'headysystems': 'https://headyio.com',\n",
    "        'headyme': 'https://headycloud.com',\n",
    "        'headyconnection': 'https://headyconnection.com',\n",
    "    },\n",
    "    'registration_endpoints': [\n",
    "        'https://headyio.com/api/nodes/register',\n",
    "        'https://headycloud.com/api/nodes/register',\n",
    "        'https://headyconnection.com/api/nodes/register',\n",
    "    ],\n",
    "    'capabilities': ['optimization', 'code_quality', 'refactor', 'security_scan',\n",
    "                     'complexity_analysis', 'similarity',\n",
    "                     'generate_text', 'reasoning_chain', 'predict', 'infer',\n",
    "                     'summarize', 'sentiment', 'classify'],\n",
    "    'heartbeat_interval_sec': 30,\n",
    "}\n",
    "\n",
    "print(f'JULES+PYTHIA Node configured')\n",
    "print(f'Capabilities: {HEADY_CONFIG[\"capabilities\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load ALL models on GPU (shared across JULES + PYTHIA)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline as hf_pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json, time, re\n",
    "from datetime import datetime\n",
    "\n",
    "device_id = 0 if torch.cuda.is_available() else -1\n",
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# JULES: Code embeddings model\n",
    "print('Loading CodeBERT embeddings (JULES)...')\n",
    "code_embedder = SentenceTransformer('microsoft/codebert-base', device=device_name)\n",
    "print('CodeBERT ready')\n",
    "\n",
    "# Shared: Text generation (used by both JULES refactoring + PYTHIA generation)\n",
    "print('Loading TinyLlama-1.1B-Chat (shared)...')\n",
    "text_generator = hf_pipeline('text-generation', model='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "                             device=device_id,\n",
    "                             torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "print('TinyLlama ready')\n",
    "\n",
    "# PYTHIA: Summarization\n",
    "print('Loading BART-large-CNN (PYTHIA summarizer)...')\n",
    "summarizer = hf_pipeline('summarization', model='facebook/bart-large-cnn', device=device_id)\n",
    "print('Summarizer ready')\n",
    "\n",
    "# PYTHIA: Sentiment\n",
    "print('Loading DistilBERT-SST2 (PYTHIA sentiment)...')\n",
    "sentiment_analyzer = hf_pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english', device=device_id)\n",
    "print('Sentiment ready')\n",
    "\n",
    "# PYTHIA: Zero-shot classification\n",
    "print('Loading BART-large-MNLI (PYTHIA classifier)...')\n",
    "classifier = hf_pipeline('zero-shot-classification', model='facebook/bart-large-mnli', device=device_id)\n",
    "print('Classifier ready')\n",
    "\n",
    "print(f'\\nAll JULES+PYTHIA models loaded on {device_name}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU Memory used: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: JULES core functions\n",
    "\n",
    "VULN_PATTERNS = {\n",
    "    'sql_injection': [r'\\$\\{.*\\}.*(?:SELECT|INSERT|UPDATE|DELETE|DROP)', r'f[\"\\'].*(?:SELECT|INSERT|UPDATE|DELETE)', r'\\+.*(?:SELECT|INSERT|UPDATE|DELETE)'],\n",
    "    'xss': [r'innerHTML\\s*=', r'document\\.write\\(', r'\\$\\(.*\\)\\.html\\(', r'dangerouslySetInnerHTML'],\n",
    "    'hardcoded_secret': [r'(?:password|secret|api_key|token)\\s*=\\s*[\"\\'][^\"\\']+(\\'w{8,})', r'(?:AWS|AZURE|GCP)_(?:SECRET|KEY)\\s*='],\n",
    "    'eval_usage': [r'eval\\(', r'exec\\(', r'Function\\('],\n",
    "    'path_traversal': [r'\\.\\./\\.\\./', ],\n",
    "    'insecure_random': [r'Math\\.random\\(\\)', r'random\\.random\\(\\)'],\n",
    "}\n",
    "\n",
    "def analyze_code_quality(code, language='javascript'):\n",
    "    \"\"\"Comprehensive code quality analysis\"\"\"\n",
    "    start = time.time()\n",
    "    lines = code.split('\\n')\n",
    "    total_lines = len(lines)\n",
    "    code_lines = len([l for l in lines if l.strip() and not l.strip().startswith(('/', '#', '*', '<!--'))])\n",
    "    comment_lines = len([l for l in lines if l.strip().startswith(('/', '#', '*', '<!--'))])\n",
    "    nesting_depths = [len(l.rstrip()) - len(l.rstrip().lstrip()) for l in lines if l.strip()]\n",
    "    max_nesting = max(nesting_depths) if nesting_depths else 0\n",
    "    avg_nesting = np.mean(nesting_depths) if nesting_depths else 0\n",
    "    func_patterns = [r'function\\s+\\w+', r'\\w+\\s*=\\s*(?:async\\s+)?(?:function|\\()', r'def\\s+\\w+', r'=>']\n",
    "    func_count = sum(len(re.findall(p, code)) for p in func_patterns)\n",
    "    vulns = []\n",
    "    for vuln_type, patterns in VULN_PATTERNS.items():\n",
    "        for pattern in patterns:\n",
    "            matches = re.findall(pattern, code, re.IGNORECASE | re.MULTILINE)\n",
    "            if matches:\n",
    "                vulns.append({'type': vuln_type, 'count': len(matches), 'severity': 'high' if vuln_type in ('sql_injection', 'hardcoded_secret') else 'medium'})\n",
    "    score = 100\n",
    "    if total_lines > 500: score -= 10\n",
    "    if max_nesting > 32: score -= 15\n",
    "    elif max_nesting > 16: score -= 8\n",
    "    if comment_lines / max(code_lines, 1) < 0.1: score -= 10\n",
    "    if func_count > 0 and code_lines / func_count > 50: score -= 10\n",
    "    score -= len(vulns) * 8\n",
    "    score = max(0, min(100, score))\n",
    "    return {\n",
    "        'quality_score': score,\n",
    "        'metrics': {'total_lines': total_lines, 'code_lines': code_lines, 'comment_lines': comment_lines,\n",
    "                    'comment_ratio': round(comment_lines / max(code_lines, 1), 3),\n",
    "                    'max_nesting_depth': max_nesting, 'avg_nesting_depth': round(avg_nesting, 2),\n",
    "                    'function_count': func_count, 'avg_function_length': round(code_lines / max(func_count, 1), 1)},\n",
    "        'vulnerabilities': vulns, 'vulnerability_count': len(vulns),\n",
    "        'language': language, 'latency_ms': round((time.time() - start) * 1000), 'device': device_name,\n",
    "    }\n",
    "\n",
    "def find_similar_code(code_blocks):\n",
    "    \"\"\"Find duplicate/similar code blocks using CodeBERT\"\"\"\n",
    "    start = time.time()\n",
    "    embeddings = code_embedder.encode(code_blocks)\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    pairs = []\n",
    "    for i in range(len(code_blocks)):\n",
    "        for j in range(i + 1, len(code_blocks)):\n",
    "            if sim_matrix[i][j] > 0.85:\n",
    "                pairs.append({'block_a': i, 'block_b': j, 'similarity': round(float(sim_matrix[i][j]), 4),\n",
    "                              'recommendation': 'Extract common logic into shared function'})\n",
    "    return {'total_blocks': len(code_blocks), 'similar_pairs': pairs, 'duplicate_count': len(pairs),\n",
    "            'latency_ms': round((time.time() - start) * 1000), 'device': device_name}\n",
    "\n",
    "def suggest_refactoring(code, language='javascript'):\n",
    "    \"\"\"Generate refactoring suggestions using shared TinyLlama\"\"\"\n",
    "    start = time.time()\n",
    "    quality = analyze_code_quality(code, language)\n",
    "    prompt = f\"\"\"You are JULES, a code optimization expert. Analyze this {language} code and provide specific refactoring suggestions.\\nCode quality score: {quality['quality_score']}/100\\nMetrics: {json.dumps(quality['metrics'])}\\nVulnerabilities: {json.dumps(quality['vulnerabilities'])}\\nCode:\\n```{language}\\n{code[:1500]}\\n```\\nProvide 3-5 specific, actionable refactoring suggestions:\"\"\"\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    formatted = text_generator.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    result = text_generator(formatted, max_new_tokens=400, temperature=0.3, do_sample=True)\n",
    "    suggestions = result[0]['generated_text'][len(formatted):].strip()\n",
    "    return {'quality': quality, 'suggestions': suggestions, 'latency_ms': round((time.time() - start) * 1000), 'device': device_name}\n",
    "\n",
    "def security_scan(code, language='javascript'):\n",
    "    \"\"\"Focused security vulnerability scan\"\"\"\n",
    "    start = time.time()\n",
    "    findings = []\n",
    "    lines = code.split('\\n')\n",
    "    for vuln_type, patterns in VULN_PATTERNS.items():\n",
    "        for pattern in patterns:\n",
    "            for i, line in enumerate(lines):\n",
    "                if re.search(pattern, line, re.IGNORECASE):\n",
    "                    findings.append({'type': vuln_type, 'line': i + 1, 'code': line.strip()[:100],\n",
    "                                     'severity': 'critical' if vuln_type in ('sql_injection', 'hardcoded_secret') else 'high' if vuln_type in ('xss', 'eval_usage') else 'medium'})\n",
    "    return {'findings': findings, 'total': len(findings),\n",
    "            'critical': len([f for f in findings if f['severity'] == 'critical']),\n",
    "            'high': len([f for f in findings if f['severity'] == 'high']),\n",
    "            'medium': len([f for f in findings if f['severity'] == 'medium']),\n",
    "            'passed': len(findings) == 0, 'latency_ms': round((time.time() - start) * 1000)}\n",
    "\n",
    "print('JULES functions ready: analyze_code_quality, find_similar_code, suggest_refactoring, security_scan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: PYTHIA core functions\n",
    "\n",
    "def generate_text(prompt, max_length=512, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"Generate text using shared TinyLlama\"\"\"\n",
    "    start = time.time()\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    formatted = text_generator.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    result = text_generator(formatted, max_new_tokens=max_length, temperature=temperature, top_p=top_p, do_sample=True)\n",
    "    generated = result[0]['generated_text'][len(formatted):].strip()\n",
    "    return {'text': generated, 'model': 'TinyLlama-1.1B-Chat', 'tokens': len(generated.split()),\n",
    "            'latency_ms': round((time.time() - start) * 1000), 'device': device_name}\n",
    "\n",
    "def reasoning_chain(question, steps=3):\n",
    "    \"\"\"Multi-step chain-of-thought reasoning\"\"\"\n",
    "    start = time.time()\n",
    "    chain = []\n",
    "    for i in range(steps):\n",
    "        step_prompt = f\"\"\"You are a reasoning engine. Think step by step.\\nQuestion: {question}\\nPrevious reasoning: {' '.join([s['thought'] for s in chain]) if chain else 'None'}\\nStep {i+1}/{steps}: Think about the next logical step.\"\"\"\n",
    "        result = generate_text(step_prompt, max_length=200)\n",
    "        chain.append({'step': i + 1, 'thought': result['text'][:500], 'latency_ms': result['latency_ms']})\n",
    "    synthesis_prompt = f\"\"\"Based on this reasoning chain, provide a final answer.\\nQuestion: {question}\\nReasoning: {json.dumps([s['thought'] for s in chain])}\\nFinal answer:\"\"\"\n",
    "    final = generate_text(synthesis_prompt, max_length=300)\n",
    "    return {'question': question, 'chain': chain, 'conclusion': final['text'], 'steps': len(chain),\n",
    "            'total_latency_ms': round((time.time() - start) * 1000), 'device': device_name}\n",
    "\n",
    "def summarize_text(text, max_length=150, min_length=30):\n",
    "    \"\"\"Summarize text using BART\"\"\"\n",
    "    start = time.time()\n",
    "    result = summarizer(text[:1024], max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return {'summary': result[0]['summary_text'], 'model': 'bart-large-cnn',\n",
    "            'input_length': len(text), 'latency_ms': round((time.time() - start) * 1000), 'device': device_name}\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Sentiment analysis\"\"\"\n",
    "    start = time.time()\n",
    "    result = sentiment_analyzer(text[:512])\n",
    "    return {'label': result[0]['label'], 'score': round(result[0]['score'], 4),\n",
    "            'model': 'distilbert-sst2', 'latency_ms': round((time.time() - start) * 1000), 'device': device_name}\n",
    "\n",
    "def classify_text(text, labels):\n",
    "    \"\"\"Zero-shot text classification\"\"\"\n",
    "    start = time.time()\n",
    "    result = classifier(text[:512], candidate_labels=labels)\n",
    "    return {'labels': result['labels'], 'scores': [round(s, 4) for s in result['scores']],\n",
    "            'top_label': result['labels'][0], 'top_score': round(result['scores'][0], 4),\n",
    "            'model': 'bart-large-mnli', 'latency_ms': round((time.time() - start) * 1000), 'device': device_name}\n",
    "\n",
    "def predict(input_data):\n",
    "    \"\"\"General prediction — routes to appropriate pipeline\"\"\"\n",
    "    task_type = input_data.get('task', 'generate')\n",
    "    if task_type == 'summarize': return summarize_text(input_data.get('text', ''))\n",
    "    elif task_type == 'sentiment': return analyze_sentiment(input_data.get('text', ''))\n",
    "    elif task_type == 'classify': return classify_text(input_data.get('text', ''), input_data.get('labels', ['positive', 'negative', 'neutral']))\n",
    "    elif task_type == 'reason': return reasoning_chain(input_data.get('question', input_data.get('text', '')), input_data.get('steps', 3))\n",
    "    else: return generate_text(input_data.get('prompt', input_data.get('text', '')), input_data.get('max_length', 512))\n",
    "\n",
    "# Quick tests\n",
    "print('=== JULES Quick Test ===')\n",
    "test_code = 'function getUserData(req) {\\n  const query = `SELECT * FROM users WHERE id = ${req.params.id}`;\\n  return db.query(query);\\n}'\n",
    "scan = security_scan(test_code)\n",
    "print(f'Security: {scan[\"total\"]} findings ({scan[\"critical\"]} critical)')\n",
    "quality = analyze_code_quality(test_code)\n",
    "print(f'Quality: {quality[\"quality_score\"]}/100')\n",
    "\n",
    "print('\\n=== PYTHIA Quick Test ===')\n",
    "sent = analyze_sentiment('Heady is building something beautiful for the world')\n",
    "print(f'Sentiment: {sent[\"label\"]} ({sent[\"score\"]:.3f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Combined FastAPI server (JULES + PYTHIA endpoints)\n",
    "from fastapi import FastAPI, Request\n",
    "import uvicorn\n",
    "import threading\n",
    "import asyncio\n",
    "import httpx\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = FastAPI(title='JULES + PYTHIA GPU Node', version='2.0.0')\n",
    "\n",
    "# ─── Health ─────────────────────────────────────────\n",
    "@app.get('/health')\n",
    "async def health():\n",
    "    return {\n",
    "        'status': 'active',\n",
    "        'node_id': HEADY_CONFIG['node_id'],\n",
    "        'node_role': HEADY_CONFIG['node_role'],\n",
    "        'combined_nodes': ['jules', 'pythia'],\n",
    "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu',\n",
    "        'gpu_memory_used_gb': round(torch.cuda.memory_allocated(0) / 1e9, 2) if torch.cuda.is_available() else 0,\n",
    "        'models': ['CodeBERT', 'TinyLlama-1.1B-Chat', 'bart-large-cnn', 'distilbert-sst2', 'bart-large-mnli'],\n",
    "        'capabilities': HEADY_CONFIG['capabilities'],\n",
    "    }\n",
    "\n",
    "# ─── JULES Endpoints ───────────────────────────────\n",
    "@app.post('/api/jules/quality')\n",
    "async def api_quality(request: Request):\n",
    "    data = await request.json()\n",
    "    return analyze_code_quality(data.get('code', ''), data.get('language', 'javascript'))\n",
    "\n",
    "@app.post('/api/jules/refactor')\n",
    "async def api_refactor(request: Request):\n",
    "    data = await request.json()\n",
    "    return suggest_refactoring(data.get('code', ''), data.get('language', 'javascript'))\n",
    "\n",
    "@app.post('/api/jules/security')\n",
    "async def api_security(request: Request):\n",
    "    data = await request.json()\n",
    "    return security_scan(data.get('code', ''), data.get('language', 'javascript'))\n",
    "\n",
    "@app.post('/api/jules/similarity')\n",
    "async def api_similarity(request: Request):\n",
    "    data = await request.json()\n",
    "    return find_similar_code(data.get('blocks', []))\n",
    "\n",
    "# ─── PYTHIA Endpoints ──────────────────────────────\n",
    "@app.post('/api/pythia/generate')\n",
    "async def api_generate(request: Request):\n",
    "    data = await request.json()\n",
    "    return generate_text(data.get('prompt', ''), data.get('max_length', 512), data.get('temperature', 0.7))\n",
    "\n",
    "@app.post('/api/pythia/reason')\n",
    "async def api_reason(request: Request):\n",
    "    data = await request.json()\n",
    "    return reasoning_chain(data.get('question', ''), data.get('steps', 3))\n",
    "\n",
    "@app.post('/api/pythia/summarize')\n",
    "async def api_pythia_summarize(request: Request):\n",
    "    data = await request.json()\n",
    "    return summarize_text(data.get('text', ''), data.get('max_length', 150))\n",
    "\n",
    "@app.post('/api/pythia/sentiment')\n",
    "async def api_sentiment(request: Request):\n",
    "    data = await request.json()\n",
    "    return analyze_sentiment(data.get('text', ''))\n",
    "\n",
    "@app.post('/api/pythia/classify')\n",
    "async def api_classify(request: Request):\n",
    "    data = await request.json()\n",
    "    return classify_text(data.get('text', ''), data.get('labels', ['positive', 'negative', 'neutral']))\n",
    "\n",
    "@app.post('/api/pythia/predict')\n",
    "async def api_predict(request: Request):\n",
    "    data = await request.json()\n",
    "    return predict(data)\n",
    "\n",
    "# ─── Universal Task Executor ───────────────────────\n",
    "@app.post('/api/tasks/execute')\n",
    "async def execute_task(request: Request):\n",
    "    data = await request.json()\n",
    "    task_type = data.get('type', 'code_quality')\n",
    "    payload = data.get('payload', data)\n",
    "    try:\n",
    "        if task_type == 'code_quality':\n",
    "            result = analyze_code_quality(payload.get('code', ''), payload.get('language', 'javascript'))\n",
    "        elif task_type in ('optimization', 'refactor'):\n",
    "            result = suggest_refactoring(payload.get('code', ''), payload.get('language', 'javascript'))\n",
    "        elif task_type == 'security_scan':\n",
    "            result = security_scan(payload.get('code', ''), payload.get('language', 'javascript'))\n",
    "        elif task_type == 'similarity':\n",
    "            result = find_similar_code(payload.get('blocks', []))\n",
    "        elif task_type in ('generate_text', 'generate'):\n",
    "            result = generate_text(payload.get('prompt', payload.get('text', '')), payload.get('max_length', 512))\n",
    "        elif task_type == 'reasoning_chain':\n",
    "            result = reasoning_chain(payload.get('question', payload.get('text', '')), payload.get('steps', 3))\n",
    "        elif task_type == 'summarize':\n",
    "            result = summarize_text(payload.get('text', ''))\n",
    "        elif task_type == 'sentiment':\n",
    "            result = analyze_sentiment(payload.get('text', ''))\n",
    "        elif task_type in ('classify', 'predict', 'infer'):\n",
    "            result = predict(payload)\n",
    "        else:\n",
    "            result = analyze_code_quality(payload.get('code', ''), payload.get('language', 'javascript'))\n",
    "        return {'success': True, 'node_id': HEADY_CONFIG['node_id'], 'task_type': task_type, 'result': result}\n",
    "    except Exception as e:\n",
    "        return {'success': False, 'error': str(e), 'node_id': HEADY_CONFIG['node_id']}\n",
    "\n",
    "print('Combined FastAPI ready with endpoints:')\n",
    "print('  JULES: /api/jules/quality, /api/jules/refactor, /api/jules/security, /api/jules/similarity')\n",
    "print('  PYTHIA: /api/pythia/generate, /api/pythia/reason, /api/pythia/summarize, /api/pythia/sentiment, /api/pythia/classify, /api/pythia/predict')\n",
    "print('  Universal: /health, /api/tasks/execute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: ngrok tunnel + auto-register with HeadyCloud\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "# Authenticate ngrok\n",
    "conf.get_default().auth_token = \"39ZBirdUD63xgta7yN7OFZpE84m_3QZyJTDno1b8Yhv9Nfy8s\"\n",
    "\n",
    "public_url = ngrok.connect(HEADY_CONFIG['port']).public_url\n",
    "print(f'JULES+PYTHIA GPU Node live at: {public_url}')\n",
    "\n",
    "PUBLIC_URL = public_url\n",
    "\n",
    "async def register_with_clouds():\n",
    "    async with httpx.AsyncClient(timeout=15) as client:\n",
    "        for endpoint in HEADY_CONFIG['registration_endpoints']:\n",
    "            try:\n",
    "                resp = await client.post(endpoint, json={\n",
    "                    'node_id': HEADY_CONFIG['node_id'],\n",
    "                    'url': PUBLIC_URL,\n",
    "                    'role': HEADY_CONFIG['node_role'],\n",
    "                    'capabilities': HEADY_CONFIG['capabilities'],\n",
    "                    'combined_nodes': ['jules', 'pythia'],\n",
    "                    'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu',\n",
    "                })\n",
    "                print(f'  Registered with {endpoint}: {resp.status_code}')\n",
    "            except Exception as e:\n",
    "                print(f'  Pending: {endpoint} ({e})')\n",
    "\n",
    "async def heartbeat_loop():\n",
    "    while True:\n",
    "        await asyncio.sleep(HEADY_CONFIG['heartbeat_interval_sec'])\n",
    "        async with httpx.AsyncClient(timeout=10) as client:\n",
    "            for layer, url in HEADY_CONFIG['cloud_layers'].items():\n",
    "                try:\n",
    "                    await client.post(f'{url}/api/nodes/heartbeat', json={\n",
    "                        'node_id': HEADY_CONFIG['node_id'],\n",
    "                        'status': 'active',\n",
    "                        'url': PUBLIC_URL,\n",
    "                        'metrics': {\n",
    "                            'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu',\n",
    "                            'gpu_memory_used': round(torch.cuda.memory_allocated(0) / 1e9, 2) if torch.cuda.is_available() else 0,\n",
    "                            'combined_nodes': ['jules', 'pythia'],\n",
    "                            'models_loaded': 5,\n",
    "                        }\n",
    "                    })\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "asyncio.run(register_with_clouds())\n",
    "threading.Thread(target=lambda: asyncio.run(heartbeat_loop()), daemon=True).start()\n",
    "print(f'JULES+PYTHIA node live at {PUBLIC_URL} — accepting code analysis & inference requests')\n",
    "# Colab VM bind address for uvicorn (dual-stack all-interfaces)\n",
    "uvicorn.run(app, host='::', port=HEADY_CONFIG['port'])"
   ]
  }
 ]
}
