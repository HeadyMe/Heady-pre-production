{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HeadySoul GPU Node — ML-Powered Mission Alignment Scoring\n",
    "\n",
    "```\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║  ∞ SACRED GEOMETRY ∞  Organic Systems · Breathing Interfaces    ║\n",
    "║  NODE: HeadySoul Colab Pro+ GPU                                 ║\n",
    "║  PURPOSE: Semantic similarity scoring against mission values     ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "```\n",
    "\n",
    "**What this does:**\n",
    "- Uses sentence-transformers on GPU to compute semantic similarity between tasks and mission values\n",
    "- Exposes a FastAPI server via ngrok tunnel\n",
    "- Auto-registers with HeadyCloud at headysystems.com\n",
    "- Sends heartbeats every 30s to stay in the cluster\n",
    "\n",
    "**Branded domains only:** headysystems.com | headycloud.com | headyconnection.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies + verify GPU\n",
    "!pip install -q sentence-transformers fastapi uvicorn pyngrok pyyaml httpx\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('WARNING: No GPU detected — running on CPU (slower)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration — Branded domains (now proxying to Render)\n",
    "import os\n",
    "\n",
    "HEADY_CONFIG = {\n",
    "    'node_id': 'heady-soul-gpu',\n",
    "    'node_role': 'soul-scorer',\n",
    "    'port': 5000,\n",
    "    'cloud_layers': {\n",
    "        'headysystems': 'https://headyio.com',\n",
    "        'headyme': 'https://headycloud.com',\n",
    "        'headyconnection': 'https://headyconnection.com',\n",
    "    },\n",
    "    'registration_endpoints': [\n",
    "        'https://headyio.com/api/soul/colab/connect',\n",
    "        'https://headycloud.com/api/soul/colab/connect',\n",
    "        'https://headyconnection.com/api/soul/colab/connect',\n",
    "    ],\n",
    "    'api_key': os.environ.get('HEADY_API_KEY', ''),\n",
    "}\n",
    "\n",
    "# Mission value weights (must match heady-soul.yaml)\n",
    "VALUE_WEIGHTS = {\n",
    "    'access': 0.30,\n",
    "    'fairness': 0.25,\n",
    "    'intelligence': 0.20,\n",
    "    'happiness': 0.15,\n",
    "    'redistribution': 0.10,\n",
    "}\n",
    "\n",
    "THRESHOLDS = {\n",
    "    'veto': 40,\n",
    "    'escalate': 60,\n",
    "    'auto_approve': 75,\n",
    "}\n",
    "\n",
    "print('HeadySoul GPU Node configured')\n",
    "print(f'Values: {VALUE_WEIGHTS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load sentence-transformer model on GPU\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "print(f'Model loaded on {device}')\n",
    "\n",
    "# Pre-compute mission dimension embeddings\n",
    "MISSION_TEXTS = {\n",
    "    'access': 'expand access to underserved users free nonprofit students PPP pricing purchasing power parity inclusive education community low-income global south multilingual offline mobile-first open-source',\n",
    "    'fairness': 'no lock-in no extraction co-ownership open transparent ethical consent privacy fair equitable democratic honest trust accountability no dark patterns portable exportable',\n",
    "    'intelligence': 'AI learning self-improving adaptive intelligent optimization pattern recognition prediction analysis insight autonomous self-correcting evolution neural network machine learning',\n",
    "    'happiness': 'user joy delight satisfaction UX user experience beautiful intuitive simple fast responsive pleasant friendly helpful supportive empowering wellbeing positive',\n",
    "    'redistribution': 'wealth redistribution revenue sharing donation profit sharing cooperative mutual aid social impact giveback scholarship subsidy pay-what-you-can sliding scale fair compensation',\n",
    "}\n",
    "\n",
    "# Sacred Geometry alignment text\n",
    "SACRED_GEOMETRY_TEXT = 'organic breathing deterministic self-correcting fractal natural rhythmic renewal healing coherent harmonic sacred geometry golden ratio fibonacci'\n",
    "\n",
    "MISSION_VECTORS = {dim: model.encode(text) for dim, text in MISSION_TEXTS.items()}\n",
    "SG_VECTOR = model.encode(SACRED_GEOMETRY_TEXT)\n",
    "\n",
    "print(f'Mission embeddings computed ({len(MISSION_VECTORS)} dimensions + Sacred Geometry)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Scoring functions\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def score_task(task):\n",
    "    \"\"\"Score task alignment with mission values using GPU-accelerated semantic similarity\"\"\"\n",
    "    task_text = f\"{task.get('title', '')} {task.get('description', '')} {task.get('type', '')} {json.dumps(task.get('metadata', {}))}\"\n",
    "    task_vec = model.encode(task_text)\n",
    "\n",
    "    breakdown = {}\n",
    "    for dim, mission_vec in MISSION_VECTORS.items():\n",
    "        sim = float(cosine_similarity([task_vec], [mission_vec])[0][0])\n",
    "        breakdown[dim] = round(max(0, min(100, sim * 100)), 2)\n",
    "\n",
    "    # Sacred Geometry bonus\n",
    "    sg_sim = float(cosine_similarity([task_vec], [SG_VECTOR])[0][0])\n",
    "    sacred_geometry = round(max(0, min(1, sg_sim)), 4)\n",
    "\n",
    "    # Weighted total\n",
    "    total = sum(breakdown[dim] * VALUE_WEIGHTS[dim] for dim in breakdown)\n",
    "    total += sacred_geometry * 0.15 * 100  # Sacred Geometry bonus (up to 15 points)\n",
    "    total = round(min(100, total), 2)\n",
    "\n",
    "    return {\n",
    "        'total': total,\n",
    "        'breakdown': breakdown,\n",
    "        'sacred_geometry': sacred_geometry,\n",
    "        'veto': total < THRESHOLDS['veto'],\n",
    "        'escalate': total >= THRESHOLDS['veto'] and total < THRESHOLDS['escalate'],\n",
    "        'auto_approve': total >= THRESHOLDS['auto_approve'],\n",
    "        'scored_by': 'colab_gpu',\n",
    "        'model': 'all-MiniLM-L6-v2',\n",
    "        'device': device,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "    }\n",
    "\n",
    "def score_strategies(strategies, context=''):\n",
    "    \"\"\"Batch score strategies for Monte Carlo filtering\"\"\"\n",
    "    results = []\n",
    "    for s in strategies:\n",
    "        desc = f\"{s.get('name', '')} {json.dumps(s.get('steps', []))} {context}\"\n",
    "        score = score_task({'description': desc, 'type': 'strategy'})\n",
    "        results.append({'strategy_id': s.get('id'), 'alignment': score['total'], 'breakdown': score['breakdown']})\n",
    "    return results\n",
    "\n",
    "# Quick test\n",
    "test_results = [\n",
    "    ('PPP pricing for nonprofits', score_task({'title': 'Add PPP pricing for nonprofits', 'description': 'Purchasing power parity tiers for underserved communities', 'metadata': {'targetUsers': ['free', 'nonprofit']}})),\n",
    "    ('Dark pattern engagement hack', score_task({'title': 'Add viral growth hack', 'description': 'Dark pattern to increase engagement and lock users in'})),\n",
    "    ('Self-improving AI pipeline', score_task({'title': 'ML pipeline optimization', 'description': 'Self-improving adaptive learning system for better predictions'})),\n",
    "]\n",
    "\n",
    "print('=== HeadySoul GPU Scoring Test ===')\n",
    "for name, result in test_results:\n",
    "    status = 'VETO' if result['veto'] else 'ESCALATE' if result['escalate'] else 'APPROVED'\n",
    "    print(f'  {name}: {result[\"total\"]:.1f}/100 [{status}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: FastAPI server\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "import uvicorn\n",
    "import threading\n",
    "import asyncio\n",
    "import httpx\n",
    "import time\n",
    "\n",
    "app = FastAPI(title='HeadySoul GPU Node', version='1.0.0')\n",
    "\n",
    "@app.get('/health')\n",
    "async def health():\n",
    "    return {\n",
    "        'status': 'active',\n",
    "        'node_id': HEADY_CONFIG['node_id'],\n",
    "        'node_role': HEADY_CONFIG['node_role'],\n",
    "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu',\n",
    "        'model': 'all-MiniLM-L6-v2',\n",
    "        'dimensions': list(VALUE_WEIGHTS.keys()),\n",
    "    }\n",
    "\n",
    "@app.post('/api/soul/evaluate')\n",
    "async def evaluate_task(request: Request):\n",
    "    task = await request.json()\n",
    "    result = score_task(task)\n",
    "    return result\n",
    "\n",
    "@app.post('/api/soul/filter-strategies')\n",
    "async def filter_strategies(request: Request):\n",
    "    data = await request.json()\n",
    "    strategies = data.get('strategies', [])\n",
    "    context = data.get('context', '')\n",
    "    min_score = data.get('minAlignmentScore', THRESHOLDS['escalate'])\n",
    "    scored = score_strategies(strategies, context)\n",
    "    approved = [s for s in scored if s['alignment'] >= min_score]\n",
    "    rejected = [s for s in scored if s['alignment'] < min_score]\n",
    "    return {'approved': approved, 'rejected': rejected, 'total': len(strategies), 'passed': len(approved)}\n",
    "\n",
    "@app.get('/api/soul/values')\n",
    "async def get_values():\n",
    "    return {'weights': VALUE_WEIGHTS, 'thresholds': THRESHOLDS}\n",
    "\n",
    "print('FastAPI app created with endpoints: /health, /api/soul/evaluate, /api/soul/filter-strategies, /api/soul/values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: ngrok tunnel + auto-register with HeadyCloud\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "# Authenticate ngrok\n",
    "conf.get_default().auth_token = \"39ZBirdUD63xgta7yN7OFZpE84m_3QZyJTDno1b8Yhv9Nfy8s\"\n",
    "\n",
    "# Start ngrok tunnel\n",
    "public_url = ngrok.connect(HEADY_CONFIG['port']).public_url\n",
    "print(f'HeadySoul GPU Node live at: {public_url}')\n",
    "\n",
    "PUBLIC_URL = public_url\n",
    "\n",
    "async def register_with_clouds():\n",
    "    \"\"\"Register this Colab GPU node with all HeadyCloud layers\"\"\"\n",
    "    async with httpx.AsyncClient(timeout=15) as client:\n",
    "        for endpoint in HEADY_CONFIG['registration_endpoints']:\n",
    "            try:\n",
    "                resp = await client.post(endpoint, json={'url': PUBLIC_URL})\n",
    "                if resp.status_code == 200:\n",
    "                    print(f'  Registered with {endpoint}: {resp.json()}')\n",
    "                else:\n",
    "                    print(f'  Registration pending: {endpoint} ({resp.status_code})')\n",
    "            except Exception as e:\n",
    "                print(f'  Could not reach {endpoint}: {e}')\n",
    "\n",
    "async def heartbeat_loop():\n",
    "    \"\"\"Send heartbeats every 30s to all cloud layers\"\"\"\n",
    "    while True:\n",
    "        await asyncio.sleep(30)\n",
    "        async with httpx.AsyncClient(timeout=10) as client:\n",
    "            for layer, url in HEADY_CONFIG['cloud_layers'].items():\n",
    "                try:\n",
    "                    await client.post(f'{url}/api/nodes/heartbeat', json={\n",
    "                        'node_id': HEADY_CONFIG['node_id'],\n",
    "                        'status': 'active',\n",
    "                        'url': PUBLIC_URL,\n",
    "                        'metrics': {\n",
    "                            'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu',\n",
    "                            'gpu_memory_used': torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0,\n",
    "                            'model': 'all-MiniLM-L6-v2',\n",
    "                        }\n",
    "                    })\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "# Register and start heartbeats\n",
    "asyncio.run(register_with_clouds())\n",
    "threading.Thread(target=lambda: asyncio.run(heartbeat_loop()), daemon=True).start()\n",
    "print(f'HeadySoul GPU node live at {PUBLIC_URL} — accepting scoring requests from HeadyCloud')\n",
    "# Colab VM bind address for uvicorn (dual-stack all-interfaces)\n",
    "uvicorn.run(app, host='::', port=HEADY_CONFIG['port'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "HeadySoul GPU Node — Mission Alignment Scorer",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
