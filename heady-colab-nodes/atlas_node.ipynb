{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HEADY COLAB PRO+ GPU NODE — ATLAS (The Auto-Archivist)\n",
        "**Sacred Geometry Architecture** · Embeddings, Semantic Search, Documentation\n",
        "\n",
        "Registers with HeadyCloud via branded domains only:\n",
        "- `headysystems.com` · `headycloud.com` · `headyconnection.com`\n",
        "\n",
        "**Setup:** Add to Colab Secrets (key icon): `HEADY_API_KEY`, `HF_TOKEN`, `NGROK_TOKEN`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!pip install -q fastapi uvicorn[standard] pyngrok nest_asyncio \\\n",
        "    sentence-transformers transformers accelerate \\\n",
        "    torch aiohttp requests scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, torch\n",
        "from datetime import datetime\n",
        "from google.colab import userdata\n",
        "\n",
        "HEADY_CONFIG = {\n",
        "    'cloud_layers': {\n",
        "        'headysystems': {'url': 'https://headyio.com', 'role': 'production'},\n",
        "        'headyme': {'url': 'https://headycloud.com', 'role': 'cloud_layer'},\n",
        "        'headyconnection': {'url': 'https://headyconnection.com', 'role': 'bridge'},\n",
        "    },\n",
        "    'heady_api_key': userdata.get('HEADY_API_KEY'),\n",
        "    'hf_token': userdata.get('HF_TOKEN'),\n",
        "    'ngrok_token': userdata.get('NGROK_TOKEN'),\n",
        "    'node_id': 'atlas',\n",
        "    'node_role': 'The Auto-Archivist',\n",
        "    'node_type': 'ai-node',\n",
        "    'cloud_layer': 'colab_pro_plus',\n",
        "    'primary_tool': 'autodoc',\n",
        "    'triggers': ['documentation', 'embeddings', 'semantic_search', 'summarize'],\n",
        "    'capabilities': {\n",
        "        'gpu': torch.cuda.is_available(),\n",
        "        'gpu_type': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'none',\n",
        "        'gpu_memory_gb': round(torch.cuda.get_device_properties(0).total_memory / 1e9, 1) if torch.cuda.is_available() else 0,\n",
        "        'max_concurrent_tasks': 8,\n",
        "        'background_execution': True,\n",
        "        'primary_functions': ['generate_embeddings', 'semantic_search', 'documentation', 'summarize'],\n",
        "    },\n",
        "    'port': 8000,\n",
        "    'heartbeat_interval_sec': 30,\n",
        "}\n",
        "\n",
        "print(f'ATLAS NODE | GPU: {HEADY_CONFIG[\"capabilities\"][\"gpu_type\"]} | VRAM: {HEADY_CONFIG[\"capabilities\"][\"gpu_memory_gb\"]} GB')\n",
        "print('Targets: headyio.com | headycloud.com | headyconnection.com')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline as hf_pipeline\n",
        "\n",
        "heady_models = {}\n",
        "heady_models['embeddings'] = SentenceTransformer('sentence-transformers/all-mpnet-base-v2').to('cuda')\n",
        "heady_models['embeddings_large'] = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2').to('cuda')\n",
        "heady_models['summarizer'] = hf_pipeline('summarization', model='facebook/bart-large-cnn', device=0, torch_dtype=torch.float16)\n",
        "print('All ATLAS models loaded to GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, HTTPException, Header\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional, Dict, Any\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "app = FastAPI(title='Heady ATLAS Node', version='3.0.0')\n",
        "node_state = {'status': 'initializing', 'tasks_completed': 0, 'tasks_failed': 0, 'uptime_start': datetime.now().isoformat(), 'current_tasks': [], 'registered_with': []}\n",
        "\n",
        "class HeadyTask(BaseModel):\n",
        "    task_id: str\n",
        "    task_type: str\n",
        "    payload: Dict[str, Any]\n",
        "    priority: str = 'P1'\n",
        "    source_cloud: str = 'unknown'\n",
        "\n",
        "@app.get('/health')\n",
        "async def health():\n",
        "    gpu_mem = torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0\n",
        "    return {'node_id': 'atlas', 'node_role': 'The Auto-Archivist', 'status': node_state['status'], 'gpu': {'type': HEADY_CONFIG['capabilities']['gpu_type'], 'used_gb': round(gpu_mem, 2), 'total_gb': HEADY_CONFIG['capabilities']['gpu_memory_gb']}, 'tasks_completed': node_state['tasks_completed'], 'current_load': len(node_state['current_tasks']), 'triggers': HEADY_CONFIG['triggers']}\n",
        "\n",
        "@app.post('/task')\n",
        "async def execute_task(task: HeadyTask, authorization: Optional[str] = Header(None)):\n",
        "    if not authorization or authorization != f'Bearer {HEADY_CONFIG[\"heady_api_key\"]}': raise HTTPException(401, 'Invalid API key')\n",
        "    start = datetime.now()\n",
        "    try:\n",
        "        node_state['current_tasks'].append(task.task_id)\n",
        "        handlers = {'generate_embeddings': do_embeddings, 'semantic_search': do_search, 'documentation': do_docs, 'summarize': do_summarize}\n",
        "        result = handlers.get(task.task_type, lambda p: (_ for _ in ()).throw(ValueError(f'Unknown: {task.task_type}')))(task.payload)\n",
        "        node_state['tasks_completed'] += 1; node_state['current_tasks'].remove(task.task_id)\n",
        "        return {'task_id': task.task_id, 'status': 'success', 'result': result, 'execution_time_ms': (datetime.now() - start).total_seconds() * 1000, 'gpu_accelerated': True}\n",
        "    except Exception as e:\n",
        "        node_state['tasks_failed'] += 1\n",
        "        if task.task_id in node_state['current_tasks']: node_state['current_tasks'].remove(task.task_id)\n",
        "        return {'task_id': task.task_id, 'status': 'failed', 'error': str(e)}\n",
        "\n",
        "def do_embeddings(p):\n",
        "    m = heady_models['embeddings_large'] if p.get('large_model') else heady_models['embeddings']\n",
        "    e = m.encode(p.get('texts', []), batch_size=64, device='cuda', convert_to_numpy=True)\n",
        "    return {'embeddings': e.tolist(), 'dimensions': e.shape[1], 'count': len(p.get('texts', []))}\n",
        "\n",
        "def do_search(p):\n",
        "    q = heady_models['embeddings'].encode([p['query']], device='cuda')\n",
        "    d = heady_models['embeddings'].encode(p.get('documents', []), batch_size=64, device='cuda')\n",
        "    s = cosine_similarity(q, d)[0]; top = s.argsort()[-p.get('top_k', 5):][::-1]\n",
        "    return {'query': p['query'], 'results': [{'document': p['documents'][i], 'score': float(s[i]), 'rank': r+1} for r, i in enumerate(top)]}\n",
        "\n",
        "def do_docs(p):\n",
        "    code = p.get('code', ''); lang = p.get('language', 'python')\n",
        "    if len(code) > 500: return {'documentation': heady_models['summarizer'](code, max_length=150, min_length=50, do_sample=False)[0]['summary_text']}\n",
        "    return {'documentation': f'```{lang}\\n{code}\\n```'}\n",
        "\n",
        "def do_summarize(p):\n",
        "    s = heady_models['summarizer'](p['text'], max_length=p.get('max_length', 150), min_length=p.get('min_length', 50), do_sample=False)\n",
        "    return {'summary': s[0]['summary_text'], 'compression_ratio': len(s[0]['summary_text']) / len(p['text'])}\n",
        "\n",
        "print('FastAPI + 4 GPU task handlers ready')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio, aiohttp, threading\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio, uvicorn\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Authenticate ngrok — hardcoded fallback if Colab Secrets not set\n",
        "try:\n",
        "    ngrok_token = HEADY_CONFIG['ngrok_token']\n",
        "except:\n",
        "    ngrok_token = \"39ZBirdUD63xgta7yN7OFZpE84m_3QZyJTDno1b8Yhv9Nfy8s\"\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "tunnel = ngrok.connect(HEADY_CONFIG['port'], 'http')\n",
        "PUBLIC_URL = tunnel.public_url\n",
        "print(f'Tunnel: {PUBLIC_URL}')\n",
        "\n",
        "async def register_with_clouds():\n",
        "    reg = {'node_id': 'atlas', 'node_type': 'ai-node', 'node_role': 'The Auto-Archivist', 'cloud_layer': 'colab_pro_plus', 'url': PUBLIC_URL, 'primary_tool': 'autodoc', 'triggers': HEADY_CONFIG['triggers'], 'capabilities': HEADY_CONFIG['capabilities']}\n",
        "    for name, cloud in HEADY_CONFIG['cloud_layers'].items():\n",
        "        for attempt in range(3):\n",
        "            try:\n",
        "                async with aiohttp.ClientSession() as session:\n",
        "                    async with session.post(f'{cloud[\"url\"]}/api/nodes/register', json=reg, headers={'Authorization': f'Bearer {HEADY_CONFIG[\"heady_api_key\"]}', 'Content-Type': 'application/json'}, timeout=aiohttp.ClientTimeout(total=30)) as resp:\n",
        "                        if resp.status == 200:\n",
        "                            node_state['registered_with'].append(name)\n",
        "                            print(f'Registered with {name} ({cloud[\"url\"]})')\n",
        "                            break\n",
        "                        else: print(f'Registration attempt {attempt+1} failed: HTTP {resp.status}')\n",
        "            except Exception as e:\n",
        "                print(f'Could not reach {name} (attempt {attempt+1}): {e}')\n",
        "                if attempt < 2: await asyncio.sleep(5)\n",
        "    node_state['status'] = 'active' if node_state['registered_with'] else 'standalone'\n",
        "    print(f'Registered with {len(node_state[\"registered_with\"])} cloud layer(s)')\n",
        "\n",
        "async def heartbeat_loop():\n",
        "    while True:\n",
        "        for name, cloud in HEADY_CONFIG['cloud_layers'].items():\n",
        "            if name not in node_state['registered_with']: continue\n",
        "            try:\n",
        "                hb = {'node_id': 'atlas', 'status': node_state['status'], 'metrics': {'tasks_completed': node_state['tasks_completed'], 'tasks_failed': node_state['tasks_failed'], 'current_load': len(node_state['current_tasks'])}}\n",
        "                async with aiohttp.ClientSession() as session:\n",
        "                    async with session.post(f'{cloud[\"url\"]}/api/nodes/heartbeat', json=hb, headers={'Authorization': f'Bearer {HEADY_CONFIG[\"heady_api_key\"]}'}, timeout=aiohttp.ClientTimeout(total=5)) as resp:\n",
        "                        if resp.status == 200: pass\n",
        "            except: pass\n",
        "        await asyncio.sleep(HEADY_CONFIG['heartbeat_interval_sec'])\n",
        "\n",
        "asyncio.run(register_with_clouds())\n",
        "threading.Thread(target=lambda: asyncio.run(heartbeat_loop()), daemon=True).start()\n",
        "print(f'ATLAS node live at {PUBLIC_URL} — accepting tasks from HeadyCloud')\n",
        "# Colab VM bind address for uvicorn (dual-stack all-interfaces)\n",
        "uvicorn.run(app, host='::', port=HEADY_CONFIG['port'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
