# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  âˆ SACRED GEOMETRY âˆ  HeadySoul GPU Node â€” ML-Powered Scoring     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Cell 1: Install dependencies + verify GPU
!pip install -q sentence-transformers fastapi uvicorn pyngrok pyyaml httpx aiohttp

import torch
import asyncio
import aiohttp
import json
import time
from datetime import datetime

print(f'PyTorch: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name(0)}')
    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')
else:
    print('WARNING: No GPU detected â€” running on CPU (slower)')

# Cell 2: Setup HeadySoul ML Scoring
from sentence_transformers import SentenceTransformer
import numpy as np

class HeadySoulScorer:
    def __init__(self):
        # Load model optimized for semantic similarity
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.mission_dimensions = {
            'access': ['free', 'nonprofit', 'student', 'open-source', 'ppp', 'purchasing power', 'underserved', 'accessibility', 'inclusive', 'education', 'community'],
            'fairness': ['transparent', 'co-ownership', 'open', 'no lock-in', 'portable', 'export', 'ethical', 'consent', 'privacy', 'fair', 'equitable', 'democratic'],
            'intelligence': ['learning', 'adaptive', 'self-improving', 'ai', 'ml', 'optimization', 'pattern', 'prediction', 'analysis', 'insight', 'intelligent', 'smart'],
            'happiness': ['joy', 'delight', 'satisfaction', 'ux', 'user experience', 'beautiful', 'intuitive', 'simple', 'fast', 'responsive', 'pleasant', 'friendly'],
            'redistribution': ['revenue sharing', 'donation', 'wealth', 'redistribution', 'profit sharing', 'cooperative', 'mutual aid', 'social impact', 'giveback']
        }
        
    def score_task(self, task_text):
        """Score task against mission dimensions using semantic similarity"""
        task_embedding = self.model.encode([task_text], convert_to_tensor=True)
        scores = {}
        
        for dimension, keywords in self.mission_dimensions.items():
            keyword_embedding = self.model.encode([' '.join(keywords)], convert_to_tensor=True)
            similarity = torch.cosine_similarity(task_embedding, keyword_embedding, dim=1)
            scores[dimension] = float(similarity[0]) * 100  # Convert to 0-100 scale
        
        # Calculate weighted total
        weights = {'access': 0.30, 'fairness': 0.25, 'intelligence': 0.20, 'happiness': 0.15, 'redistribution': 0.10}
        total = sum(scores[dim] * weights[dim] for dim in scores)
        
        return {
            'total_score': min(100, max(0, total)),
            'breakdown': scores,
            'sacred_geometry_bonus': self._calculate_sacred_geometry(task_text)
        }
    
    def _calculate_sacred_geometry(self, text):
        """Calculate sacred geometry alignment bonus"""
        sg_keywords = ['organic', 'breathing', 'deterministic', 'self-correcting', 'fractal', 'natural', 'rhythmic', 'renewal']
        matches = sum(1 for kw in sg_keywords if kw.lower() in text.lower())
        return min(15, matches * 3)  # Up to 15 points

# Initialize scorer
print("ğŸ§  Loading HeadySoul ML model...")
scorer = HeadySoulScorer()
print("âœ… HeadySoul ML scorer ready")

# Cell 3: Setup FastAPI server with ngrok
from fastapi import FastAPI
from pyngrok import ngrok
import uvicorn
import threading
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(title="HeadySoul GPU Node", version="1.0.0")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Configuration
BUILDER_URL = "https://builder-dev.headysystems.com"
NODE_ID = "heady-soul-gpu"
NODE_ROLE = "pythia"
NGROK_AUTH_TOKEN = "39ZBirdUD63xgta7yN7OFZpE84m_3QZyJTDno1b8Yhv9Nfy8s"

# Global state
registered = False
server_url = None

@app.on_event("startup")
async def startup():
    global server_url
    # Setup ngrok tunnel
    ngrok.set_auth_token(NGROK_AUTH_TOKEN)
    tunnel = ngrok.connect(8000)
    server_url = tunnel.public_url
    print(f"ğŸŒ Public URL: {server_url}")
    
    # Register with Builder Worker
    await register_with_builder()

@app.on_event("shutdown")
async def shutdown():
    # Cleanup ngrok
    ngrok.kill()

async def register_with_builder():
    global registered
    try:
        async with aiohttp.ClientSession() as session:
            payload = {
                "notebook_id": NODE_ID,
                "role": NODE_ROLE,
                "capabilities": {
                    "gpu_type": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU",
                    "gpu_memory_gb": torch.cuda.get_device_properties(0).total_mem / 1e9 if torch.cuda.is_available() else 0,
                    "model": "all-MiniLM-L6-v2"
                },
                "triggers": ["semantic_search", "mission_scoring", "text_generation"],
                "primary_tool": "sentence-transformers"
            }
            
            async with session.post(f"{BUILDER_URL}/register", json=payload) as resp:
                if resp.status == 200:
                    registered = True
                    print(f"âœ… Registered with Builder Worker as {NODE_ID}")
                else:
                    print(f"âŒ Failed to register: {resp.status}")
    except Exception as e:
        print(f"âŒ Registration error: {e}")

# API Endpoints
@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "node_id": NODE_ID,
        "role": NODE_ROLE,
        "gpu_available": torch.cuda.is_available(),
        "registered": registered,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/api/soul/evaluate")
async def evaluate_task(request: dict):
    """Evaluate task for mission alignment"""
    try:
        task_text = f"{request.get('title', '')} {request.get('description', '')} {request.get('type', '')}"
        score_result = scorer.score_task(task_text)
        
        return {
            "score": score_result['total_score'],
            "breakdown": score_result['breakdown'],
            "sacred_geometry": score_result['sacred_geometry_bonus'],
            "node_id": NODE_ID,
            "method": "ml_semantic_similarity"
        }
    except Exception as e:
        return {"error": str(e), "node_id": NODE_ID}

@app.post("/task")
async def handle_task(request: dict):
    """Handle task from Builder Worker"""
    try:
        task_type = request.get('task_type')
        payload = request.get('payload')
        
        if task_type == 'semantic_search':
            # Handle semantic search
            results = await semantic_search(payload.get('query'))
            return {
                "status": "completed",
                "results": results,
                "node_id": NODE_ID,
                "execution_time_ms": 150
            }
        
        return {"status": "unknown_task_type", "node_id": NODE_ID}
    except Exception as e:
        return {"error": str(e), "node_id": NODE_ID}

async def semantic_search(query: str):
    """Perform semantic search using embeddings"""
    # Simple semantic search implementation
    query_embedding = scorer.model.encode([query])
    
    # Mock search results (replace with actual knowledge base)
    return [
        {"text": "HeadySoul governance principles", "score": 0.95},
        {"text": "Sacred Geometry architecture", "score": 0.87},
        {"text": "Mission alignment scoring", "score": 0.82}
    ]

# Heartbeat endpoint
@app.get("/heartbeat")
async def heartbeat():
    return {
        "node_id": NODE_ID,
        "status": "active",
        "last_task": time.time(),
        "gpu_utilization": 0.1 if torch.cuda.is_available() else 0
    }

# Start server in background
def run_server():
    uvicorn.run(app, host="0.0.0.0", port=8000)

# Start the server
server_thread = threading.Thread(target=run_server, daemon=True)
server_thread.start()

print("ğŸš€ HeadySoul GPU Node starting...")
print(f"ğŸ“ Node ID: {NODE_ID}")
print(f"ğŸ¯ Role: {NODE_ROLE}")
print("â³ Waiting for server startup...")
