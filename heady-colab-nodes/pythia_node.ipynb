{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYTHIA GPU Node — Reasoning Chains & HuggingFace Inference\n",
    "\n",
    "```\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║  ∞ SACRED GEOMETRY ∞  Organic Systems · Breathing Interfaces    ║\n",
    "║  NODE: PYTHIA Colab Pro+ GPU                                    ║\n",
    "║  PURPOSE: Text generation, reasoning chains, prediction         ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "```\n",
    "\n",
    "**Capabilities:**\n",
    "- Text generation & completion via HuggingFace models\n",
    "- Multi-step reasoning chains with chain-of-thought\n",
    "- Prediction & inference pipelines\n",
    "- Semantic analysis & summarization\n",
    "\n",
    "**Branded domains only:** headysystems.com | headycloud.com | headyconnection.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies + verify GPU\n",
    "!pip install -q transformers accelerate sentencepiece fastapi uvicorn pyngrok httpx torch\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('WARNING: No GPU — will run on CPU (slower)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "import os\n",
    "\n",
    "HEADY_CONFIG = {\n",
    "    'node_id': 'pythia-gpu',\n",
    "    'node_role': 'reasoning-engine',\n",
    "    'port': 5001,\n",
    "    'cloud_layers': {\n",
    "        'headysystems': 'https://headysystems.com',\n",
    "        'headyme': 'https://headycloud.com',\n",
    "        'headyconnection': 'https://headyconnection.com',\n",
    "    },\n",
    "    'registration_endpoints': [\n",
    "        'https://headysystems.com/api/nodes/register',\n",
    "        'https://headycloud.com/api/nodes/register',\n",
    "        'https://headyconnection.com/api/nodes/register',\n",
    "    ],\n",
    "    'hf_token': os.environ.get('HF_TOKEN', ''),\n",
    "    'capabilities': ['generate_text', 'reasoning_chain', 'huggingface', 'predict', 'infer', 'summarize'],\n",
    "}\n",
    "\n",
    "# Task types this node handles\n",
    "SUPPORTED_TASKS = {\n",
    "    'generate_text': 'Text generation and completion',\n",
    "    'reasoning_chain': 'Multi-step chain-of-thought reasoning',\n",
    "    'predict': 'Prediction from input features',\n",
    "    'infer': 'Inference on structured/unstructured data',\n",
    "    'summarize': 'Text summarization',\n",
    "    'sentiment': 'Sentiment analysis',\n",
    "    'classify': 'Text classification',\n",
    "}\n",
    "\n",
    "print(f'PYTHIA Node configured')\n",
    "print(f'Capabilities: {HEADY_CONFIG[\"capabilities\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load models on GPU\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load pipelines\n",
    "print('Loading text generation model...')\n",
    "text_generator = pipeline('text-generation', model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', device=device, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "print('Text generation ready')\n",
    "\n",
    "print('Loading summarization model...')\n",
    "summarizer = pipeline('summarization', model='facebook/bart-large-cnn', device=device)\n",
    "print('Summarization ready')\n",
    "\n",
    "print('Loading sentiment model...')\n",
    "sentiment_analyzer = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english', device=device)\n",
    "print('Sentiment analysis ready')\n",
    "\n",
    "print('Loading zero-shot classifier...')\n",
    "classifier = pipeline('zero-shot-classification', model='facebook/bart-large-mnli', device=device)\n",
    "print('Zero-shot classification ready')\n",
    "\n",
    "print(f'\\nAll PYTHIA models loaded on {device_name}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU Memory used: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Core PYTHIA functions\n",
    "import time\n",
    "\n",
    "def generate_text(prompt, max_length=512, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"Generate text from prompt using TinyLlama\"\"\"\n",
    "    start = time.time()\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    formatted = text_generator.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    result = text_generator(formatted, max_new_tokens=max_length, temperature=temperature, top_p=top_p, do_sample=True)\n",
    "    generated = result[0]['generated_text'][len(formatted):].strip()\n",
    "    elapsed = time.time() - start\n",
    "    return {\n",
    "        'text': generated,\n",
    "        'model': 'TinyLlama-1.1B-Chat',\n",
    "        'tokens': len(generated.split()),\n",
    "        'latency_ms': round(elapsed * 1000),\n",
    "        'device': device_name,\n",
    "    }\n",
    "\n",
    "def reasoning_chain(question, steps=3):\n",
    "    \"\"\"Multi-step chain-of-thought reasoning\"\"\"\n",
    "    start = time.time()\n",
    "    chain = []\n",
    "    context = question\n",
    "\n",
    "    for i in range(steps):\n",
    "        step_prompt = f\"\"\"You are a reasoning engine. Think step by step.\n",
    "Question: {question}\n",
    "Previous reasoning: {' '.join([s['thought'] for s in chain]) if chain else 'None'}\n",
    "Step {i+1}/{steps}: Think about the next logical step.\"\"\"\n",
    "        result = generate_text(step_prompt, max_length=200)\n",
    "        chain.append({\n",
    "            'step': i + 1,\n",
    "            'thought': result['text'][:500],\n",
    "            'latency_ms': result['latency_ms'],\n",
    "        })\n",
    "\n",
    "    # Final synthesis\n",
    "    synthesis_prompt = f\"\"\"Based on this reasoning chain, provide a final answer.\n",
    "Question: {question}\n",
    "Reasoning: {json.dumps([s['thought'] for s in chain])}\n",
    "Final answer:\"\"\"\n",
    "    final = generate_text(synthesis_prompt, max_length=300)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    return {\n",
    "        'question': question,\n",
    "        'chain': chain,\n",
    "        'conclusion': final['text'],\n",
    "        'steps': len(chain),\n",
    "        'total_latency_ms': round(elapsed * 1000),\n",
    "        'device': device_name,\n",
    "    }\n",
    "\n",
    "def summarize_text(text, max_length=150, min_length=30):\n",
    "    \"\"\"Summarize text using BART\"\"\"\n",
    "    start = time.time()\n",
    "    result = summarizer(text[:1024], max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    elapsed = time.time() - start\n",
    "    return {\n",
    "        'summary': result[0]['summary_text'],\n",
    "        'model': 'bart-large-cnn',\n",
    "        'input_length': len(text),\n",
    "        'latency_ms': round(elapsed * 1000),\n",
    "        'device': device_name,\n",
    "    }\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Sentiment analysis\"\"\"\n",
    "    start = time.time()\n",
    "    result = sentiment_analyzer(text[:512])\n",
    "    elapsed = time.time() - start\n",
    "    return {\n",
    "        'label': result[0]['label'],\n",
    "        'score': round(result[0]['score'], 4),\n",
    "        'model': 'distilbert-sst2',\n",
    "        'latency_ms': round(elapsed * 1000),\n",
    "        'device': device_name,\n",
    "    }\n",
    "\n",
    "def classify_text(text, labels):\n",
    "    \"\"\"Zero-shot text classification\"\"\"\n",
    "    start = time.time()\n",
    "    result = classifier(text[:512], candidate_labels=labels)\n",
    "    elapsed = time.time() - start\n",
    "    return {\n",
    "        'labels': result['labels'],\n",
    "        'scores': [round(s, 4) for s in result['scores']],\n",
    "        'top_label': result['labels'][0],\n",
    "        'top_score': round(result['scores'][0], 4),\n",
    "        'model': 'bart-large-mnli',\n",
    "        'latency_ms': round(elapsed * 1000),\n",
    "        'device': device_name,\n",
    "    }\n",
    "\n",
    "def predict(input_data):\n",
    "    \"\"\"General prediction — routes to appropriate pipeline\"\"\"\n",
    "    task_type = input_data.get('task', 'generate')\n",
    "    if task_type == 'summarize':\n",
    "        return summarize_text(input_data.get('text', ''))\n",
    "    elif task_type == 'sentiment':\n",
    "        return analyze_sentiment(input_data.get('text', ''))\n",
    "    elif task_type == 'classify':\n",
    "        return classify_text(input_data.get('text', ''), input_data.get('labels', ['positive', 'negative', 'neutral']))\n",
    "    elif task_type == 'reason':\n",
    "        return reasoning_chain(input_data.get('question', input_data.get('text', '')), input_data.get('steps', 3))\n",
    "    else:\n",
    "        return generate_text(input_data.get('prompt', input_data.get('text', '')), input_data.get('max_length', 512))\n",
    "\n",
    "# Quick test\n",
    "print('=== PYTHIA Quick Test ===')\n",
    "print('Sentiment:', analyze_sentiment('Heady is building something beautiful for the world'))\n",
    "print('Classify:', classify_text('Add PPP pricing for nonprofits', ['access', 'fairness', 'revenue', 'technical']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: FastAPI server\n",
    "from fastapi import FastAPI, Request\n",
    "import uvicorn\n",
    "import threading\n",
    "import asyncio\n",
    "import httpx\n",
    "\n",
    "app = FastAPI(title='PYTHIA GPU Node', version='1.0.0')\n",
    "\n",
    "@app.get('/health')\n",
    "async def health():\n",
    "    return {\n",
    "        'status': 'active',\n",
    "        'node_id': HEADY_CONFIG['node_id'],\n",
    "        'node_role': HEADY_CONFIG['node_role'],\n",
    "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu',\n",
    "        'gpu_memory_used_gb': round(torch.cuda.memory_allocated(0) / 1e9, 2) if torch.cuda.is_available() else 0,\n",
    "        'models': ['TinyLlama-1.1B-Chat', 'bart-large-cnn', 'distilbert-sst2', 'bart-large-mnli'],\n",
    "        'capabilities': HEADY_CONFIG['capabilities'],\n",
    "        'supported_tasks': list(SUPPORTED_TASKS.keys()),\n",
    "    }\n",
    "\n",
    "@app.post('/api/pythia/generate')\n",
    "async def api_generate(request: Request):\n",
    "    data = await request.json()\n",
    "    return generate_text(data.get('prompt', ''), data.get('max_length', 512), data.get('temperature', 0.7))\n",
    "\n",
    "@app.post('/api/pythia/reason')\n",
    "async def api_reason(request: Request):\n",
    "    data = await request.json()\n",
    "    return reasoning_chain(data.get('question', ''), data.get('steps', 3))\n",
    "\n",
    "@app.post('/api/pythia/summarize')\n",
    "async def api_summarize(request: Request):\n",
    "    data = await request.json()\n",
    "    return summarize_text(data.get('text', ''), data.get('max_length', 150))\n",
    "\n",
    "@app.post('/api/pythia/sentiment')\n",
    "async def api_sentiment(request: Request):\n",
    "    data = await request.json()\n",
    "    return analyze_sentiment(data.get('text', ''))\n",
    "\n",
    "@app.post('/api/pythia/classify')\n",
    "async def api_classify(request: Request):\n",
    "    data = await request.json()\n",
    "    return classify_text(data.get('text', ''), data.get('labels', ['positive', 'negative', 'neutral']))\n",
    "\n",
    "@app.post('/api/pythia/predict')\n",
    "async def api_predict(request: Request):\n",
    "    data = await request.json()\n",
    "    return predict(data)\n",
    "\n",
    "@app.post('/api/tasks/execute')\n",
    "async def execute_task(request: Request):\n",
    "    \"\"\"Universal task executor — compatible with HeadyCloud task routing\"\"\"\n",
    "    data = await request.json()\n",
    "    task_type = data.get('type', 'generate_text')\n",
    "    payload = data.get('payload', data)\n",
    "    try:\n",
    "        if task_type in ('generate_text', 'generate'):\n",
    "            result = generate_text(payload.get('prompt', payload.get('text', '')), payload.get('max_length', 512))\n",
    "        elif task_type == 'reasoning_chain':\n",
    "            result = reasoning_chain(payload.get('question', payload.get('text', '')), payload.get('steps', 3))\n",
    "        elif task_type == 'summarize':\n",
    "            result = summarize_text(payload.get('text', ''))\n",
    "        elif task_type == 'sentiment':\n",
    "            result = analyze_sentiment(payload.get('text', ''))\n",
    "        elif task_type in ('classify', 'predict', 'infer'):\n",
    "            result = predict(payload)\n",
    "        else:\n",
    "            result = generate_text(json.dumps(payload)[:500])\n",
    "        return {'success': True, 'node_id': HEADY_CONFIG['node_id'], 'task_type': task_type, 'result': result}\n",
    "    except Exception as e:\n",
    "        return {'success': False, 'error': str(e), 'node_id': HEADY_CONFIG['node_id']}\n",
    "\n",
    "print('PYTHIA FastAPI ready: /health, /api/pythia/generate, /api/pythia/reason, /api/pythia/summarize, /api/pythia/sentiment, /api/pythia/classify, /api/pythia/predict, /api/tasks/execute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: ngrok tunnel + auto-register with HeadyCloud\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "# Authenticate ngrok\n",
    "conf.get_default().auth_token = \"39ZBirdUD63xgta7yN7OFZpE84m_3QZyJTDno1b8Yhv9Nfy8s\"\n",
    "\n",
    "public_url = ngrok.connect(HEADY_CONFIG['port']).public_url\n",
    "print(f'PYTHIA GPU Node live at: {public_url}')\n",
    "\n",
    "PUBLIC_URL = public_url\n",
    "\n",
    "async def register_with_clouds():\n",
    "    async with httpx.AsyncClient(timeout=15) as client:\n",
    "        for endpoint in HEADY_CONFIG['registration_endpoints']:\n",
    "            try:\n",
    "                resp = await client.post(endpoint, json={\n",
    "                    'node_id': HEADY_CONFIG['node_id'],\n",
    "                    'url': PUBLIC_URL,\n",
    "                    'role': HEADY_CONFIG['node_role'],\n",
    "                    'capabilities': HEADY_CONFIG['capabilities'],\n",
    "                    'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu',\n",
    "                })\n",
    "                print(f'  Registered with {endpoint}: {resp.status_code}')\n",
    "            except Exception as e:\n",
    "                print(f'  Pending: {endpoint} ({e})')\n",
    "\n",
    "async def heartbeat_loop():\n",
    "    while True:\n",
    "        await asyncio.sleep(30)\n",
    "        async with httpx.AsyncClient(timeout=10) as client:\n",
    "            for layer, url in HEADY_CONFIG['cloud_layers'].items():\n",
    "                try:\n",
    "                    await client.post(f'{url}/api/nodes/heartbeat', json={\n",
    "                        'node_id': HEADY_CONFIG['node_id'],\n",
    "                        'status': 'active',\n",
    "                        'url': PUBLIC_URL,\n",
    "                        'metrics': {\n",
    "                            'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu',\n",
    "                            'gpu_memory_used': round(torch.cuda.memory_allocated(0) / 1e9, 2) if torch.cuda.is_available() else 0,\n",
    "                            'models_loaded': 4,\n",
    "                        }\n",
    "                    })\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "asyncio.run(register_with_clouds())\n",
    "threading.Thread(target=lambda: asyncio.run(heartbeat_loop()), daemon=True).start()\n",
    "print(f'PYTHIA GPU node live at {PUBLIC_URL} — accepting inference requests')\n",
    "# 0.0.0.0 is the Colab VM bind address for uvicorn, NOT a local service\n",
    "uvicorn.run(app, host='0.0.0.0', port=HEADY_CONFIG['port'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "PYTHIA GPU Node — Reasoning & Inference Engine",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
