{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HeadySoul + ATLAS GPU Node — Mission Scoring & Semantic Intelligence\n",
    "\n",
    "```\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║  ∞ SACRED GEOMETRY ∞  Organic Systems · Breathing Interfaces    ║\n",
    "║  NODE: HeadySoul + ATLAS Combined (Colab Pro+ GPU)              ║\n",
    "║  PURPOSE: Mission alignment scoring + semantic search + docs    ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "```\n",
    "\n",
    "**Combined capabilities (2-in-1 GPU node):**\n",
    "- HeadySoul: Mission alignment scoring via semantic similarity\n",
    "- HeadySoul: Strategy filtering for Monte Carlo engine\n",
    "- ATLAS: Embedding generation (mpnet + MiniLM)\n",
    "- ATLAS: Semantic search across documents\n",
    "- ATLAS: Code documentation & summarization\n",
    "\n",
    "**Branded domains only:** headysystems.com | headycloud.com | headyconnection.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies + verify GPU\n",
    "!pip install -q sentence-transformers fastapi uvicorn pyngrok pyyaml httpx scikit-learn aiohttp nest_asyncio\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('WARNING: No GPU detected — running on CPU (slower)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "import os\n",
    "\n",
    "HEADY_CONFIG = {\n",
    "    'node_id': 'soul-atlas-gpu',\n",
    "    'node_role': 'soul-scorer-archivist',\n",
    "    'port': 5000,\n",
    "    'cloud_layers': {\n",
    "        'headysystems': 'https://headyio.com',\n",
    "        'headyme': 'https://headycloud.com',\n",
    "        'headyconnection': 'https://headyconnection.com',\n",
    "    },\n",
    "    'registration_endpoints': [\n",
    "        'https://headyio.com/api/soul/colab/connect',\n",
    "        'https://headycloud.com/api/soul/colab/connect',\n",
    "        'https://headyconnection.com/api/soul/colab/connect',\n",
    "        'https://headyio.com/api/nodes/register',\n",
    "        'https://headycloud.com/api/nodes/register',\n",
    "        'https://headyconnection.com/api/nodes/register',\n",
    "    ],\n",
    "    'api_key': os.environ.get('HEADY_API_KEY', ''),\n",
    "    'capabilities': ['soul_scoring', 'embeddings', 'semantic_search', 'documentation', 'summarize'],\n",
    "    'heartbeat_interval_sec': 30,\n",
    "}\n",
    "\n",
    "# HeadySoul mission value weights (must match heady-soul.yaml)\n",
    "VALUE_WEIGHTS = {\n",
    "    'access': 0.30,\n",
    "    'fairness': 0.25,\n",
    "    'intelligence': 0.20,\n",
    "    'happiness': 0.15,\n",
    "    'redistribution': 0.10,\n",
    "}\n",
    "\n",
    "THRESHOLDS = {\n",
    "    'veto': 40,\n",
    "    'escalate': 60,\n",
    "    'auto_approve': 75,\n",
    "}\n",
    "\n",
    "print('HeadySoul + ATLAS combined node configured')\n",
    "print(f'Values: {VALUE_WEIGHTS}')\n",
    "print(f'Capabilities: {HEADY_CONFIG[\"capabilities\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load ALL models on GPU (shared across HeadySoul + ATLAS)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline as hf_pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json, time\n",
    "from datetime import datetime\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Shared embedding model (used by both HeadySoul and ATLAS)\n",
    "print('Loading shared embedding model (all-MiniLM-L6-v2)...')\n",
    "model_minilm = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# ATLAS additional models\n",
    "print('Loading ATLAS mpnet model (all-mpnet-base-v2)...')\n",
    "model_mpnet = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "print('Loading ATLAS large model (all-MiniLM-L12-v2)...')\n",
    "model_large = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2', device=device)\n",
    "\n",
    "print('Loading summarizer (bart-large-cnn)...')\n",
    "summarizer = hf_pipeline('summarization', model='facebook/bart-large-cnn', device=0 if torch.cuda.is_available() else -1, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "\n",
    "# Pre-compute HeadySoul mission dimension embeddings\n",
    "MISSION_TEXTS = {\n",
    "    'access': 'expand access to underserved users free nonprofit students PPP pricing purchasing power parity inclusive education community low-income global south multilingual offline mobile-first open-source',\n",
    "    'fairness': 'no lock-in no extraction co-ownership open transparent ethical consent privacy fair equitable democratic honest trust accountability no dark patterns portable exportable',\n",
    "    'intelligence': 'AI learning self-improving adaptive intelligent optimization pattern recognition prediction analysis insight autonomous self-correcting evolution neural network machine learning',\n",
    "    'happiness': 'user joy delight satisfaction UX user experience beautiful intuitive simple fast responsive pleasant friendly helpful supportive empowering wellbeing positive',\n",
    "    'redistribution': 'wealth redistribution revenue sharing donation profit sharing cooperative mutual aid social impact giveback scholarship subsidy pay-what-you-can sliding scale fair compensation',\n",
    "}\n",
    "SACRED_GEOMETRY_TEXT = 'organic breathing deterministic self-correcting fractal natural rhythmic renewal healing coherent harmonic sacred geometry golden ratio fibonacci'\n",
    "\n",
    "MISSION_VECTORS = {dim: model_minilm.encode(text) for dim, text in MISSION_TEXTS.items()}\n",
    "SG_VECTOR = model_minilm.encode(SACRED_GEOMETRY_TEXT)\n",
    "\n",
    "print(f'\\nAll models loaded on {device}')\n",
    "print(f'Mission embeddings: {len(MISSION_VECTORS)} dimensions + Sacred Geometry')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU Memory used: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: HeadySoul scoring functions\n",
    "\n",
    "def score_task(task):\n",
    "    \"\"\"Score task alignment with mission values using GPU-accelerated semantic similarity\"\"\"\n",
    "    task_text = f\"{task.get('title', '')} {task.get('description', '')} {task.get('type', '')} {json.dumps(task.get('metadata', {}))}\"\n",
    "    task_vec = model_minilm.encode(task_text)\n",
    "\n",
    "    breakdown = {}\n",
    "    for dim, mission_vec in MISSION_VECTORS.items():\n",
    "        sim = float(cosine_similarity([task_vec], [mission_vec])[0][0])\n",
    "        breakdown[dim] = round(max(0, min(100, sim * 100)), 2)\n",
    "\n",
    "    sg_sim = float(cosine_similarity([task_vec], [SG_VECTOR])[0][0])\n",
    "    sacred_geometry = round(max(0, min(1, sg_sim)), 4)\n",
    "\n",
    "    total = sum(breakdown[dim] * VALUE_WEIGHTS[dim] for dim in breakdown)\n",
    "    total += sacred_geometry * 0.15 * 100\n",
    "    total = round(min(100, total), 2)\n",
    "\n",
    "    return {\n",
    "        'total': total,\n",
    "        'breakdown': breakdown,\n",
    "        'sacred_geometry': sacred_geometry,\n",
    "        'veto': total < THRESHOLDS['veto'],\n",
    "        'escalate': total >= THRESHOLDS['veto'] and total < THRESHOLDS['escalate'],\n",
    "        'auto_approve': total >= THRESHOLDS['auto_approve'],\n",
    "        'scored_by': 'colab_gpu',\n",
    "        'model': 'all-MiniLM-L6-v2',\n",
    "        'device': device,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "    }\n",
    "\n",
    "def score_strategies(strategies, context=''):\n",
    "    \"\"\"Batch score strategies for Monte Carlo filtering\"\"\"\n",
    "    results = []\n",
    "    for s in strategies:\n",
    "        desc = f\"{s.get('name', '')} {json.dumps(s.get('steps', []))} {context}\"\n",
    "        score = score_task({'description': desc, 'type': 'strategy'})\n",
    "        results.append({'strategy_id': s.get('id'), 'alignment': score['total'], 'breakdown': score['breakdown']})\n",
    "    return results\n",
    "\n",
    "# ATLAS functions\n",
    "def do_embeddings(payload):\n",
    "    \"\"\"Generate embeddings for texts\"\"\"\n",
    "    m = model_large if payload.get('large_model') else model_mpnet\n",
    "    texts = payload.get('texts', [])\n",
    "    e = m.encode(texts, batch_size=64, device=device, convert_to_numpy=True)\n",
    "    return {'embeddings': e.tolist(), 'dimensions': e.shape[1], 'count': len(texts)}\n",
    "\n",
    "def do_search(payload):\n",
    "    \"\"\"Semantic search across documents\"\"\"\n",
    "    q = model_mpnet.encode([payload['query']], device=device)\n",
    "    docs = payload.get('documents', [])\n",
    "    d = model_mpnet.encode(docs, batch_size=64, device=device)\n",
    "    s = cosine_similarity(q, d)[0]\n",
    "    top_k = payload.get('top_k', 5)\n",
    "    top = s.argsort()[-top_k:][::-1]\n",
    "    return {\n",
    "        'query': payload['query'],\n",
    "        'results': [{'document': docs[i], 'score': float(s[i]), 'rank': r+1} for r, i in enumerate(top)]\n",
    "    }\n",
    "\n",
    "def do_docs(payload):\n",
    "    \"\"\"Generate documentation for code\"\"\"\n",
    "    code = payload.get('code', '')\n",
    "    lang = payload.get('language', 'python')\n",
    "    if len(code) > 500:\n",
    "        return {'documentation': summarizer(code, max_length=150, min_length=50, do_sample=False)[0]['summary_text']}\n",
    "    return {'documentation': f'```{lang}\\n{code}\\n```'}\n",
    "\n",
    "def do_summarize(payload):\n",
    "    \"\"\"Summarize text\"\"\"\n",
    "    s = summarizer(payload['text'], max_length=payload.get('max_length', 150), min_length=payload.get('min_length', 50), do_sample=False)\n",
    "    return {'summary': s[0]['summary_text'], 'compression_ratio': len(s[0]['summary_text']) / len(payload['text'])}\n",
    "\n",
    "# Quick test\n",
    "test_results = [\n",
    "    ('PPP pricing for nonprofits', score_task({'title': 'Add PPP pricing for nonprofits', 'description': 'Purchasing power parity tiers for underserved communities', 'metadata': {'targetUsers': ['free', 'nonprofit']}})),\n",
    "    ('Dark pattern engagement hack', score_task({'title': 'Add viral growth hack', 'description': 'Dark pattern to increase engagement and lock users in'})),\n",
    "]\n",
    "\n",
    "print('=== HeadySoul Scoring Test ===')\n",
    "for name, result in test_results:\n",
    "    status = 'VETO' if result['veto'] else 'ESCALATE' if result['escalate'] else 'APPROVED'\n",
    "    print(f'  {name}: {result[\"total\"]:.1f}/100 [{status}]')\n",
    "\n",
    "print('\\n=== ATLAS Embedding Test ===')\n",
    "test_embed = do_embeddings({'texts': ['test sentence']})\n",
    "print(f'  Dimensions: {test_embed[\"dimensions\"]}, Count: {test_embed[\"count\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Combined FastAPI server (HeadySoul + ATLAS endpoints)\n",
    "from fastapi import FastAPI, Request, HTTPException, Header\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, Dict, Any, List\n",
    "import uvicorn\n",
    "import threading\n",
    "import asyncio\n",
    "import httpx\n",
    "import aiohttp\n",
    "\n",
    "app = FastAPI(title='HeadySoul + ATLAS GPU Node', version='2.0.0')\n",
    "node_state = {\n",
    "    'status': 'initializing',\n",
    "    'tasks_completed': 0,\n",
    "    'tasks_failed': 0,\n",
    "    'uptime_start': datetime.now().isoformat(),\n",
    "    'current_tasks': [],\n",
    "    'registered_with': [],\n",
    "}\n",
    "\n",
    "class HeadyTask(BaseModel):\n",
    "    task_id: str\n",
    "    task_type: str\n",
    "    payload: Dict[str, Any]\n",
    "    priority: str = 'P1'\n",
    "    source_cloud: str = 'unknown'\n",
    "\n",
    "# ─── Health ─────────────────────────────────────────\n",
    "@app.get('/health')\n",
    "async def health():\n",
    "    gpu_mem = torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0\n",
    "    return {\n",
    "        'status': node_state['status'],\n",
    "        'node_id': HEADY_CONFIG['node_id'],\n",
    "        'node_role': HEADY_CONFIG['node_role'],\n",
    "        'combined_nodes': ['heady-soul', 'atlas'],\n",
    "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu',\n",
    "        'gpu_memory_used_gb': round(gpu_mem, 2),\n",
    "        'models': ['all-MiniLM-L6-v2', 'all-mpnet-base-v2', 'all-MiniLM-L12-v2', 'bart-large-cnn'],\n",
    "        'capabilities': HEADY_CONFIG['capabilities'],\n",
    "        'dimensions': list(VALUE_WEIGHTS.keys()),\n",
    "        'tasks_completed': node_state['tasks_completed'],\n",
    "        'current_load': len(node_state['current_tasks']),\n",
    "    }\n",
    "\n",
    "# ─── HeadySoul Endpoints ────────────────────────────\n",
    "@app.post('/api/soul/evaluate')\n",
    "async def evaluate_task(request: Request):\n",
    "    task = await request.json()\n",
    "    return score_task(task)\n",
    "\n",
    "@app.post('/api/soul/filter-strategies')\n",
    "async def filter_strategies(request: Request):\n",
    "    data = await request.json()\n",
    "    strategies = data.get('strategies', [])\n",
    "    context = data.get('context', '')\n",
    "    min_score = data.get('minAlignmentScore', THRESHOLDS['escalate'])\n",
    "    scored = score_strategies(strategies, context)\n",
    "    approved = [s for s in scored if s['alignment'] >= min_score]\n",
    "    rejected = [s for s in scored if s['alignment'] < min_score]\n",
    "    return {'approved': approved, 'rejected': rejected, 'total': len(strategies), 'passed': len(approved)}\n",
    "\n",
    "@app.get('/api/soul/values')\n",
    "async def get_values():\n",
    "    return {'weights': VALUE_WEIGHTS, 'thresholds': THRESHOLDS}\n",
    "\n",
    "# ─── ATLAS Endpoints ───────────────────────────────\n",
    "@app.post('/api/atlas/embeddings')\n",
    "async def api_embeddings(request: Request):\n",
    "    data = await request.json()\n",
    "    return do_embeddings(data)\n",
    "\n",
    "@app.post('/api/atlas/search')\n",
    "async def api_search(request: Request):\n",
    "    data = await request.json()\n",
    "    return do_search(data)\n",
    "\n",
    "@app.post('/api/atlas/docs')\n",
    "async def api_docs(request: Request):\n",
    "    data = await request.json()\n",
    "    return do_docs(data)\n",
    "\n",
    "@app.post('/api/atlas/summarize')\n",
    "async def api_summarize(request: Request):\n",
    "    data = await request.json()\n",
    "    return do_summarize(data)\n",
    "\n",
    "# ─── Universal Task Executor ───────────────────────\n",
    "@app.post('/task')\n",
    "async def execute_task_atlas(task: HeadyTask, authorization: Optional[str] = Header(None)):\n",
    "    start = datetime.now()\n",
    "    try:\n",
    "        node_state['current_tasks'].append(task.task_id)\n",
    "        handlers = {\n",
    "            'generate_embeddings': do_embeddings,\n",
    "            'semantic_search': do_search,\n",
    "            'documentation': do_docs,\n",
    "            'summarize': do_summarize,\n",
    "            'soul_evaluate': lambda p: score_task(p),\n",
    "            'soul_filter': lambda p: {'results': score_strategies(p.get('strategies', []), p.get('context', ''))},\n",
    "        }\n",
    "        handler = handlers.get(task.task_type)\n",
    "        if not handler:\n",
    "            raise ValueError(f'Unknown task type: {task.task_type}')\n",
    "        result = handler(task.payload)\n",
    "        node_state['tasks_completed'] += 1\n",
    "        node_state['current_tasks'].remove(task.task_id)\n",
    "        return {'task_id': task.task_id, 'status': 'success', 'result': result,\n",
    "                'execution_time_ms': (datetime.now() - start).total_seconds() * 1000, 'gpu_accelerated': True}\n",
    "    except Exception as e:\n",
    "        node_state['tasks_failed'] += 1\n",
    "        if task.task_id in node_state['current_tasks']:\n",
    "            node_state['current_tasks'].remove(task.task_id)\n",
    "        return {'task_id': task.task_id, 'status': 'failed', 'error': str(e)}\n",
    "\n",
    "@app.post('/api/tasks/execute')\n",
    "async def execute_task_generic(request: Request):\n",
    "    data = await request.json()\n",
    "    task_type = data.get('type', 'soul_evaluate')\n",
    "    payload = data.get('payload', data)\n",
    "    try:\n",
    "        if task_type in ('soul_evaluate', 'soul_scoring'):\n",
    "            result = score_task(payload)\n",
    "        elif task_type == 'soul_filter':\n",
    "            result = {'results': score_strategies(payload.get('strategies', []), payload.get('context', ''))}\n",
    "        elif task_type in ('generate_embeddings', 'embeddings'):\n",
    "            result = do_embeddings(payload)\n",
    "        elif task_type == 'semantic_search':\n",
    "            result = do_search(payload)\n",
    "        elif task_type == 'documentation':\n",
    "            result = do_docs(payload)\n",
    "        elif task_type == 'summarize':\n",
    "            result = do_summarize(payload)\n",
    "        else:\n",
    "            result = score_task(payload)\n",
    "        return {'success': True, 'node_id': HEADY_CONFIG['node_id'], 'task_type': task_type, 'result': result}\n",
    "    except Exception as e:\n",
    "        return {'success': False, 'error': str(e), 'node_id': HEADY_CONFIG['node_id']}\n",
    "\n",
    "print('Combined FastAPI ready with endpoints:')\n",
    "print('  HeadySoul: /api/soul/evaluate, /api/soul/filter-strategies, /api/soul/values')\n",
    "print('  ATLAS: /api/atlas/embeddings, /api/atlas/search, /api/atlas/docs, /api/atlas/summarize')\n",
    "print('  Universal: /health, /task, /api/tasks/execute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: ngrok tunnel + auto-register with all HeadyCloud layers\n",
    "from pyngrok import ngrok, conf\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Authenticate ngrok\n",
    "conf.get_default().auth_token = \"39ZBirdUD63xgta7yN7OFZpE84m_3QZyJTDno1b8Yhv9Nfy8s\"\n",
    "\n",
    "# Start ngrok tunnel\n",
    "public_url = ngrok.connect(HEADY_CONFIG['port']).public_url\n",
    "print(f'HeadySoul+ATLAS GPU Node live at: {public_url}')\n",
    "\n",
    "PUBLIC_URL = public_url\n",
    "\n",
    "async def register_with_clouds():\n",
    "    \"\"\"Register this combined node with all HeadyCloud layers\"\"\"\n",
    "    reg_data = {\n",
    "        'node_id': HEADY_CONFIG['node_id'],\n",
    "        'node_type': 'ai-node',\n",
    "        'node_role': HEADY_CONFIG['node_role'],\n",
    "        'cloud_layer': 'colab_pro_plus',\n",
    "        'url': PUBLIC_URL,\n",
    "        'primary_tool': 'soul-scoring-and-autodoc',\n",
    "        'triggers': ['soul', 'evaluate', 'score', 'embeddings', 'semantic_search', 'documentation', 'summarize'],\n",
    "        'capabilities': {\n",
    "            'gpu': torch.cuda.is_available(),\n",
    "            'gpu_type': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'none',\n",
    "            'combined_nodes': ['heady-soul', 'atlas'],\n",
    "            'primary_functions': ['score_task', 'filter_strategies', 'generate_embeddings', 'semantic_search', 'documentation', 'summarize'],\n",
    "        },\n",
    "    }\n",
    "    async with httpx.AsyncClient(timeout=15) as client:\n",
    "        for endpoint in HEADY_CONFIG['registration_endpoints']:\n",
    "            try:\n",
    "                resp = await client.post(endpoint, json=reg_data)\n",
    "                if resp.status_code == 200:\n",
    "                    node_state['registered_with'].append(endpoint)\n",
    "                    print(f'  Registered with {endpoint}: {resp.status_code}')\n",
    "                else:\n",
    "                    print(f'  Registration pending: {endpoint} ({resp.status_code})')\n",
    "            except Exception as e:\n",
    "                print(f'  Could not reach {endpoint}: {e}')\n",
    "    node_state['status'] = 'active' if node_state['registered_with'] else 'standalone'\n",
    "\n",
    "async def heartbeat_loop():\n",
    "    \"\"\"Send heartbeats every 30s to all cloud layers\"\"\"\n",
    "    while True:\n",
    "        await asyncio.sleep(HEADY_CONFIG['heartbeat_interval_sec'])\n",
    "        async with httpx.AsyncClient(timeout=10) as client:\n",
    "            for layer, url in HEADY_CONFIG['cloud_layers'].items():\n",
    "                try:\n",
    "                    await client.post(f'{url}/api/nodes/heartbeat', json={\n",
    "                        'node_id': HEADY_CONFIG['node_id'],\n",
    "                        'status': node_state['status'],\n",
    "                        'url': PUBLIC_URL,\n",
    "                        'metrics': {\n",
    "                            'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu',\n",
    "                            'gpu_memory_used': torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0,\n",
    "                            'combined_nodes': ['heady-soul', 'atlas'],\n",
    "                            'tasks_completed': node_state['tasks_completed'],\n",
    "                            'current_load': len(node_state['current_tasks']),\n",
    "                        }\n",
    "                    })\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "# Register and start heartbeats\n",
    "asyncio.run(register_with_clouds())\n",
    "threading.Thread(target=lambda: asyncio.run(heartbeat_loop()), daemon=True).start()\n",
    "print(f'HeadySoul+ATLAS node live at {PUBLIC_URL} — accepting scoring & search requests')\n",
    "# Colab VM bind address for uvicorn (dual-stack all-interfaces)\n",
    "uvicorn.run(app, host='::', port=HEADY_CONFIG['port'])"
   ]
  }
 ]
}
