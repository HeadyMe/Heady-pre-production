{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDER GPU Node — Project Build, Hydration & Scaffolding\n",
    "\n",
    "```\n",
    "╔══════════════════════════════════════════════════════════════════╗\n",
    "║  ∞ SACRED GEOMETRY ∞  Organic Systems · Breathing Interfaces    ║\n",
    "║  NODE: BUILDER Colab Pro+ GPU                                   ║\n",
    "║  PURPOSE: Project builds, hydration, scaffolding, dependency    ║\n",
    "║           analysis, architecture generation                      ║\n",
    "╚══════════════════════════════════════════════════════════════════╝\n",
    "```\n",
    "\n",
    "**Capabilities:**\n",
    "- Project scaffolding from templates via ML-assisted generation\n",
    "- Dependency graph analysis & conflict detection\n",
    "- Build optimization recommendations\n",
    "- Architecture diagram generation from code structure\n",
    "- Hydration — enrich partial projects with missing components\n",
    "\n",
    "**Branded domains only:** headysystems.com | headycloud.com | headyconnection.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies + verify GPU\n",
    "!pip install -q transformers accelerate sentence-transformers fastapi uvicorn pyngrok httpx networkx\n",
    "\n",
    "import torch\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\n",
    "else:\n",
    "    print('WARNING: No GPU — will run on CPU (slower)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "import os\n",
    "\n",
    "HEADY_CONFIG = {\n",
    "    'node_id': 'builder-gpu',\n",
    "    'node_role': 'project-builder',\n",
    "    'port': 5003,\n",
    "    'cloud_layers': {\n",
    "        'headysystems': 'https://heady-manager-headysystems.onrender.com',\n",
    "        'headyme': 'https://heady-manager-headyme.onrender.com',\n",
    "        'headyconnection': 'https://heady-manager-headyconnection.onrender.com',\n",
    "    },\n",
    "    'registration_endpoints': [\n",
    "        'https://heady-manager-headysystems.onrender.com/api/nodes/register',\n",
    "        'https://heady-manager-headyme.onrender.com/api/nodes/register',\n",
    "        'https://heady-manager-headyconnection.onrender.com/api/nodes/register',\n",
    "    ],\n",
    "    'capabilities': ['build_project', 'newproject', 'hydrate', 'dependency_analysis', 'architecture'],\n",
    "}\n",
    "\n",
    "SUPPORTED_TASKS = {\n",
    "    'build_project': 'Analyze and optimize project build process',\n",
    "    'newproject': 'Scaffold a new project from description',\n",
    "    'hydrate': 'Enrich partial project with missing components',\n",
    "    'dependency_analysis': 'Analyze dependency graph, detect conflicts',\n",
    "    'architecture': 'Generate architecture analysis from file structure',\n",
    "}\n",
    "\n",
    "# Project templates\n",
    "TEMPLATES = {\n",
    "    'node-api': {\n",
    "        'files': ['package.json', 'server.js', 'src/routes/index.js', 'src/middleware/auth.js', 'src/models/index.js', '.env.example', 'README.md', '.gitignore'],\n",
    "        'deps': ['express', 'cors', 'helmet', 'compression', 'dotenv'],\n",
    "        'dev_deps': ['nodemon', 'jest'],\n",
    "    },\n",
    "    'react-app': {\n",
    "        'files': ['package.json', 'src/App.jsx', 'src/index.jsx', 'src/components/Layout.jsx', 'public/index.html', 'tailwind.config.js', '.env.example', 'README.md'],\n",
    "        'deps': ['react', 'react-dom', 'react-router-dom', 'lucide-react'],\n",
    "        'dev_deps': ['vite', '@vitejs/plugin-react', 'tailwindcss', 'postcss', 'autoprefixer'],\n",
    "    },\n",
    "    'python-api': {\n",
    "        'files': ['requirements.txt', 'app.py', 'src/__init__.py', 'src/routes.py', 'src/models.py', 'src/middleware.py', '.env.example', 'README.md', 'Dockerfile'],\n",
    "        'deps': ['fastapi', 'uvicorn', 'pydantic', 'sqlalchemy', 'python-dotenv'],\n",
    "        'dev_deps': ['pytest', 'httpx'],\n",
    "    },\n",
    "    'fullstack': {\n",
    "        'files': ['package.json', 'server.js', 'src/routes/api.js', 'frontend/package.json', 'frontend/src/App.jsx', 'frontend/src/index.jsx', 'docker-compose.yml', '.env.example', 'README.md'],\n",
    "        'deps': ['express', 'cors', 'helmet', 'react', 'react-dom'],\n",
    "        'dev_deps': ['vite', 'nodemon', 'concurrently'],\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f'BUILDER Node configured')\n",
    "print(f'Capabilities: {HEADY_CONFIG[\"capabilities\"]}')\n",
    "print(f'Templates: {list(TEMPLATES.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json, time, re\n",
    "from datetime import datetime\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('Loading code embedding model...')\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "print('Embeddings ready')\n",
    "\n",
    "print('Loading generation model...')\n",
    "generator = pipeline('text-generation', model='TinyLlama/TinyLlama-1.1B-Chat-v1.0', device=0 if torch.cuda.is_available() else -1, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "print('Generation ready')\n",
    "\n",
    "# Pre-compute template embeddings for smart matching\n",
    "template_descriptions = {\n",
    "    'node-api': 'backend REST API server Node.js Express endpoint routes database',\n",
    "    'react-app': 'frontend React web application UI components dashboard',\n",
    "    'python-api': 'backend Python FastAPI server machine learning data science',\n",
    "    'fullstack': 'full stack web application frontend backend database deployment',\n",
    "}\n",
    "TEMPLATE_VECTORS = {k: embedder.encode(v) for k, v in template_descriptions.items()}\n",
    "\n",
    "print(f'\\nAll BUILDER models loaded on {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU Memory used: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Core BUILDER functions\n",
    "\n",
    "def analyze_dependencies(deps, dev_deps=None):\n",
    "    \"\"\"Analyze dependency graph for conflicts and optimization opportunities\"\"\"\n",
    "    start = time.time()\n",
    "    all_deps = deps + (dev_deps or [])\n",
    "\n",
    "    # Build dependency graph\n",
    "    G = nx.DiGraph()\n",
    "    for dep in all_deps:\n",
    "        name = dep.split('@')[0] if '@' in dep else dep\n",
    "        G.add_node(name, type='prod' if dep in deps else 'dev')\n",
    "\n",
    "    # Known dependency relationships (common ones)\n",
    "    KNOWN_DEPS = {\n",
    "        'react-dom': ['react'],\n",
    "        'react-router-dom': ['react', 'react-dom'],\n",
    "        '@vitejs/plugin-react': ['vite', 'react'],\n",
    "        'tailwindcss': ['postcss', 'autoprefixer'],\n",
    "        'express': ['http-errors', 'content-type'],\n",
    "        'helmet': ['express'],\n",
    "        'cors': ['express'],\n",
    "        'compression': ['express'],\n",
    "    }\n",
    "\n",
    "    missing = []\n",
    "    for dep in all_deps:\n",
    "        name = dep.split('@')[0] if '@' in dep else dep\n",
    "        if name in KNOWN_DEPS:\n",
    "            for peer in KNOWN_DEPS[name]:\n",
    "                G.add_edge(name, peer)\n",
    "                if peer not in [d.split('@')[0] if '@' in d else d for d in all_deps]:\n",
    "                    missing.append({'package': peer, 'required_by': name, 'type': 'peer_dependency'})\n",
    "\n",
    "    # Check for circular dependencies\n",
    "    cycles = list(nx.simple_cycles(G))\n",
    "\n",
    "    # Compute importance (PageRank)\n",
    "    try:\n",
    "        importance = nx.pagerank(G)\n",
    "        critical = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    except:\n",
    "        critical = []\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    return {\n",
    "        'total_deps': len(deps),\n",
    "        'total_dev_deps': len(dev_deps or []),\n",
    "        'missing_peers': missing,\n",
    "        'circular_deps': cycles,\n",
    "        'critical_packages': [{'package': k, 'importance': round(v, 4)} for k, v in critical],\n",
    "        'graph_nodes': G.number_of_nodes(),\n",
    "        'graph_edges': G.number_of_edges(),\n",
    "        'latency_ms': round(elapsed * 1000),\n",
    "    }\n",
    "\n",
    "def scaffold_project(description, preferred_template=None):\n",
    "    \"\"\"Scaffold a new project from natural language description\"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    # Smart template matching via embeddings\n",
    "    if not preferred_template:\n",
    "        desc_vec = embedder.encode(description)\n",
    "        similarities = {k: float(cosine_similarity([desc_vec], [v])[0][0]) for k, v in TEMPLATE_VECTORS.items()}\n",
    "        preferred_template = max(similarities, key=similarities.get)\n",
    "        match_confidence = similarities[preferred_template]\n",
    "    else:\n",
    "        match_confidence = 1.0\n",
    "\n",
    "    template = TEMPLATES.get(preferred_template, TEMPLATES['node-api'])\n",
    "\n",
    "    # Generate project structure with LLM\n",
    "    prompt = f\"\"\"You are BUILDER, a project scaffolding expert. Create a project structure for:\n",
    "{description}\n",
    "\n",
    "Template: {preferred_template}\n",
    "Required files: {json.dumps(template['files'])}\n",
    "Dependencies: {json.dumps(template['deps'])}\n",
    "\n",
    "Generate a brief description of what each file should contain and any additional files needed:\"\"\"\n",
    "\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    formatted = generator.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    result = generator(formatted, max_new_tokens=500, temperature=0.4, do_sample=True)\n",
    "    structure = result[0]['generated_text'][len(formatted):].strip()\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    return {\n",
    "        'template': preferred_template,\n",
    "        'match_confidence': round(match_confidence, 4),\n",
    "        'files': template['files'],\n",
    "        'dependencies': template['deps'],\n",
    "        'dev_dependencies': template['dev_deps'],\n",
    "        'structure_guide': structure,\n",
    "        'dep_analysis': analyze_dependencies(template['deps'], template['dev_deps']),\n",
    "        'latency_ms': round(elapsed * 1000),\n",
    "        'device': device,\n",
    "    }\n",
    "\n",
    "def hydrate_project(existing_files, project_type=None):\n",
    "    \"\"\"Analyze existing project and suggest missing components\"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    # Detect project type from files\n",
    "    if not project_type:\n",
    "        if any('package.json' in f for f in existing_files):\n",
    "            if any('.jsx' in f or '.tsx' in f for f in existing_files):\n",
    "                project_type = 'react-app'\n",
    "            else:\n",
    "                project_type = 'node-api'\n",
    "        elif any('requirements.txt' in f or 'setup.py' in f for f in existing_files):\n",
    "            project_type = 'python-api'\n",
    "        else:\n",
    "            project_type = 'fullstack'\n",
    "\n",
    "    template = TEMPLATES.get(project_type, TEMPLATES['node-api'])\n",
    "    existing_basenames = [f.split('/')[-1] for f in existing_files]\n",
    "\n",
    "    missing = []\n",
    "    for template_file in template['files']:\n",
    "        basename = template_file.split('/')[-1]\n",
    "        if basename not in existing_basenames:\n",
    "            missing.append(template_file)\n",
    "\n",
    "    # Essential files check\n",
    "    essential_missing = []\n",
    "    essentials = ['.gitignore', 'README.md', '.env.example']\n",
    "    for e in essentials:\n",
    "        if e not in existing_basenames:\n",
    "            essential_missing.append(e)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    return {\n",
    "        'detected_type': project_type,\n",
    "        'existing_files': len(existing_files),\n",
    "        'template_files': len(template['files']),\n",
    "        'missing_files': missing,\n",
    "        'missing_count': len(missing),\n",
    "        'essential_missing': essential_missing,\n",
    "        'completeness': round((1 - len(missing) / max(len(template['files']), 1)) * 100, 1),\n",
    "        'recommended_deps': template['deps'],\n",
    "        'latency_ms': round(elapsed * 1000),\n",
    "    }\n",
    "\n",
    "def analyze_architecture(files, file_contents=None):\n",
    "    \"\"\"Generate architecture analysis from file structure\"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    # Categorize files\n",
    "    categories = {\n",
    "        'routes': [], 'models': [], 'middleware': [], 'components': [],\n",
    "        'utils': [], 'config': [], 'tests': [], 'styles': [], 'other': [],\n",
    "    }\n",
    "\n",
    "    for f in files:\n",
    "        lower = f.lower()\n",
    "        if any(x in lower for x in ['route', 'api', 'endpoint']): categories['routes'].append(f)\n",
    "        elif any(x in lower for x in ['model', 'schema', 'entity']): categories['models'].append(f)\n",
    "        elif any(x in lower for x in ['middleware', 'auth', 'guard']): categories['middleware'].append(f)\n",
    "        elif any(x in lower for x in ['component', '.jsx', '.tsx', '.vue']): categories['components'].append(f)\n",
    "        elif any(x in lower for x in ['util', 'helper', 'lib']): categories['utils'].append(f)\n",
    "        elif any(x in lower for x in ['config', '.env', '.yaml', '.yml']): categories['config'].append(f)\n",
    "        elif any(x in lower for x in ['test', 'spec', '__test__']): categories['tests'].append(f)\n",
    "        elif any(x in lower for x in ['.css', '.scss', '.less', 'style']): categories['styles'].append(f)\n",
    "        else: categories['other'].append(f)\n",
    "\n",
    "    # Compute metrics\n",
    "    dirs = set()\n",
    "    extensions = {}\n",
    "    for f in files:\n",
    "        parts = f.split('/')\n",
    "        if len(parts) > 1:\n",
    "            dirs.add(parts[0])\n",
    "        ext = f.rsplit('.', 1)[-1] if '.' in f else 'no_ext'\n",
    "        extensions[ext] = extensions.get(ext, 0) + 1\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    return {\n",
    "        'total_files': len(files),\n",
    "        'directories': len(dirs),\n",
    "        'categories': {k: {'count': len(v), 'files': v} for k, v in categories.items() if v},\n",
    "        'extensions': dict(sorted(extensions.items(), key=lambda x: x[1], reverse=True)),\n",
    "        'has_tests': len(categories['tests']) > 0,\n",
    "        'test_coverage_estimate': round(len(categories['tests']) / max(len(files), 1) * 100, 1),\n",
    "        'latency_ms': round(elapsed * 1000),\n",
    "    }\n",
    "\n",
    "# Quick test\n",
    "print('=== BUILDER Quick Test ===')\n",
    "scaffold = scaffold_project('A real-time dashboard for monitoring AI agent performance')\n",
    "print(f'Template matched: {scaffold[\"template\"]} (confidence: {scaffold[\"match_confidence\"]})')\n",
    "print(f'Files: {len(scaffold[\"files\"])}, Deps: {len(scaffold[\"dependencies\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: FastAPI server\n",
    "from fastapi import FastAPI, Request\n",
    "import uvicorn\n",
    "import threading\n",
    "import asyncio\n",
    "import httpx\n",
    "\n",
    "app = FastAPI(title='BUILDER GPU Node', version='1.0.0')\n",
    "\n",
    "@app.get('/health')\n",
    "async def health():\n",
    "    return {\n",
    "        'status': 'active',\n",
    "        'node_id': HEADY_CONFIG['node_id'],\n",
    "        'node_role': HEADY_CONFIG['node_role'],\n",
    "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu',\n",
    "        'gpu_memory_used_gb': round(torch.cuda.memory_allocated(0) / 1e9, 2) if torch.cuda.is_available() else 0,\n",
    "        'models': ['all-MiniLM-L6-v2', 'TinyLlama-1.1B-Chat'],\n",
    "        'capabilities': HEADY_CONFIG['capabilities'],\n",
    "        'supported_tasks': list(SUPPORTED_TASKS.keys()),\n",
    "        'templates': list(TEMPLATES.keys()),\n",
    "    }\n",
    "\n",
    "@app.post('/api/builder/scaffold')\n",
    "async def api_scaffold(request: Request):\n",
    "    data = await request.json()\n",
    "    return scaffold_project(data.get('description', ''), data.get('template', None))\n",
    "\n",
    "@app.post('/api/builder/hydrate')\n",
    "async def api_hydrate(request: Request):\n",
    "    data = await request.json()\n",
    "    return hydrate_project(data.get('files', []), data.get('project_type', None))\n",
    "\n",
    "@app.post('/api/builder/dependencies')\n",
    "async def api_deps(request: Request):\n",
    "    data = await request.json()\n",
    "    return analyze_dependencies(data.get('deps', []), data.get('dev_deps', []))\n",
    "\n",
    "@app.post('/api/builder/architecture')\n",
    "async def api_arch(request: Request):\n",
    "    data = await request.json()\n",
    "    return analyze_architecture(data.get('files', []))\n",
    "\n",
    "@app.post('/api/tasks/execute')\n",
    "async def execute_task(request: Request):\n",
    "    data = await request.json()\n",
    "    task_type = data.get('type', 'build_project')\n",
    "    payload = data.get('payload', data)\n",
    "    try:\n",
    "        if task_type in ('build_project', 'dependency_analysis'):\n",
    "            result = analyze_dependencies(payload.get('deps', []), payload.get('dev_deps', []))\n",
    "        elif task_type == 'newproject':\n",
    "            result = scaffold_project(payload.get('description', ''), payload.get('template', None))\n",
    "        elif task_type == 'hydrate':\n",
    "            result = hydrate_project(payload.get('files', []), payload.get('project_type', None))\n",
    "        elif task_type == 'architecture':\n",
    "            result = analyze_architecture(payload.get('files', []))\n",
    "        else:\n",
    "            result = scaffold_project(json.dumps(payload)[:300])\n",
    "        return {'success': True, 'node_id': HEADY_CONFIG['node_id'], 'task_type': task_type, 'result': result}\n",
    "    except Exception as e:\n",
    "        return {'success': False, 'error': str(e), 'node_id': HEADY_CONFIG['node_id']}\n",
    "\n",
    "print('BUILDER FastAPI ready: /health, /api/builder/scaffold, /api/builder/hydrate, /api/builder/dependencies, /api/builder/architecture, /api/tasks/execute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: ngrok tunnel + auto-register with HeadyCloud\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "# Authenticate ngrok\n",
    "conf.get_default().auth_token = \"39ZBirdUD63xgta7yN7OFZpE84m_3QZyJTDno1b8Yhv9Nfy8s\"\n",
    "\n",
    "public_url = ngrok.connect(HEADY_CONFIG['port']).public_url\n",
    "print(f'BUILDER GPU Node live at: {public_url}')\n",
    "\n",
    "PUBLIC_URL = public_url\n",
    "\n",
    "async def register_with_clouds():\n",
    "    async with httpx.AsyncClient(timeout=15) as client:\n",
    "        for endpoint in HEADY_CONFIG['registration_endpoints']:\n",
    "            try:\n",
    "                resp = await client.post(endpoint, json={\n",
    "                    'node_id': HEADY_CONFIG['node_id'],\n",
    "                    'url': PUBLIC_URL,\n",
    "                    'role': HEADY_CONFIG['node_role'],\n",
    "                    'capabilities': HEADY_CONFIG['capabilities'],\n",
    "                    'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu',\n",
    "                })\n",
    "                print(f'  Registered with {endpoint}: {resp.status_code}')\n",
    "            except Exception as e:\n",
    "                print(f'  Pending: {endpoint} ({e})')\n",
    "\n",
    "async def heartbeat_loop():\n",
    "    while True:\n",
    "        await asyncio.sleep(30)\n",
    "        async with httpx.AsyncClient(timeout=10) as client:\n",
    "            for layer, url in HEADY_CONFIG['cloud_layers'].items():\n",
    "                try:\n",
    "                    await client.post(f'{url}/api/nodes/heartbeat', json={\n",
    "                        'node_id': HEADY_CONFIG['node_id'],\n",
    "                        'status': 'active',\n",
    "                        'url': PUBLIC_URL,\n",
    "                        'metrics': {\n",
    "                            'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'cpu',\n",
    "                            'gpu_memory_used': round(torch.cuda.memory_allocated(0) / 1e9, 2) if torch.cuda.is_available() else 0,\n",
    "                            'models_loaded': 2,\n",
    "                        }\n",
    "                    })\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "asyncio.run(register_with_clouds())\n",
    "threading.Thread(target=lambda: asyncio.run(heartbeat_loop()), daemon=True).start()\n",
    "print(f'BUILDER GPU node live at {PUBLIC_URL} — accepting build requests')\n",
    "# 0.0.0.0 is the Colab VM bind address for uvicorn, NOT a local service\n",
    "uvicorn.run(app, host='0.0.0.0', port=HEADY_CONFIG['port'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "BUILDER GPU Node — Project Build & Hydration Engine",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
